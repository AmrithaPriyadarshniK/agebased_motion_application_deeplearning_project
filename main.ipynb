{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/misbah4064/age_and_gender_detection/blob/master/age%26genderDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9H_w4CXIC_h","executionInfo":{"status":"ok","timestamp":1734533543657,"user_tz":-330,"elapsed":23034,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a318541b-61e7-4332-a1a4-1ba0d863ab8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/drive/.file-revisions-by-id': Operation canceled\n","rm: cannot remove '/content/drive/MyDrive': Operation canceled\n","rm: cannot remove '/content/drive/.shortcut-targets-by-id': Operation canceled\n","rm: cannot remove '/content/drive/.Trash-0': Directory not empty\n","rm: cannot remove '/content/drive/.Encrypted/MyDrive': Operation canceled\n","rm: cannot remove '/content/drive/.Encrypted/.shortcut-targets-by-id': Operation canceled\n"]}],"source":["!rm -rf /content/drive\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10984,"status":"ok","timestamp":1734761199142,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"},"user_tz":-330},"id":"dKOGIOCZqxMD","outputId":"dfadee8d-f3ca-4528-fc90-6214878c4d45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"39L6WYZ176AQ","outputId":"6a0bd315-5111-4ac7-bf19-f1f68da6d6c1","executionInfo":{"status":"ok","timestamp":1734761266554,"user_tz":-330,"elapsed":60275,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.23.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Collecting numpy>=1.21.2 (from opencv-python-headless)\n","  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n","Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","Installing collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.3\n","    Uninstalling numpy-1.23.3:\n","      Successfully uninstalled numpy-1.23.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 1.4.20 requires scipy>=1.10.0, but you have scipy 1.9.1 which is incompatible.\n","bigframes 1.29.0 requires matplotlib>=3.7.1, but you have matplotlib 3.6.0 which is incompatible.\n","bigframes 1.29.0 requires pandas>=1.5.3, but you have pandas 1.5.0 which is incompatible.\n","bigframes 1.29.0 requires pytz>=2022.7, but you have pytz 2022.2.1 which is incompatible.\n","bigframes 1.29.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.2 which is incompatible.\n","cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.0 which is incompatible.\n","ibis-framework 9.2.0 requires pandas<3,>=1.5.3, but you have pandas 1.5.0 which is incompatible.\n","ibis-framework 9.2.0 requires pytz>=2022.7, but you have pytz 2022.2.1 which is incompatible.\n","jax 0.4.33 requires scipy>=1.10, but you have scipy 1.9.1 which is incompatible.\n","jaxlib 0.4.33 requires scipy>=1.10, but you have scipy 1.9.1 which is incompatible.\n","mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.0 which is incompatible.\n","mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.1.2 which is incompatible.\n","peft 0.14.0 requires torch>=1.13.0, but you have torch 1.12.1 which is incompatible.\n","plotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.6.0 which is incompatible.\n","plotnine 0.14.4 requires pandas>=2.2.0, but you have pandas 1.5.0 which is incompatible.\n","scipy 1.9.1 requires numpy<1.25.0,>=1.18.5, but you have numpy 1.26.4 which is incompatible.\n","xarray 2024.11.0 requires pandas>=2.1, but you have pandas 1.5.0 which is incompatible.\n","yfinance 0.2.50 requires pytz>=2022.5, but you have pytz 2022.2.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.4\n","Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.20)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n","Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.6.0)\n","Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n","Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n","Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n","Collecting scipy>=1.10 (from jax->mediapipe)\n","  Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n","Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.9.1\n","    Uninstalling scipy-1.9.1:\n","      Successfully uninstalled scipy-1.9.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n","mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.0 which is incompatible.\n","mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.1.2 which is incompatible.\n","plotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.6.0 which is incompatible.\n","plotnine 0.14.4 requires pandas>=2.2.0, but you have pandas 1.5.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed scipy-1.14.1\n","Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.9.1)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\n","Requirement already satisfied: gradio-client==1.5.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.2)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.0)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0)\n","Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.4)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n","Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.64.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2022.2.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<3.0,>=1.0->gradio) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Requirement already satisfied: rembg in /usr/local/lib/python3.10/dist-packages (2.0.61)\n","Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.20.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg) (4.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.26.4)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from rembg) (4.10.0.84)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from rembg) (11.0.0)\n","Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from rembg) (1.8.2)\n","Requirement already satisfied: pymatting in /usr/local/lib/python3.10/dist-packages (from rembg) (1.1.13)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from rembg) (0.19.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.14.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rembg) (4.64.1)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (4.25.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (24.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.22.3)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (4.3.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (2.32.3)\n","Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting->rembg) (0.60.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (3.4.2)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2.22.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2024.12.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (1.8.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting->rembg) (0.43.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2024.12.14)\n","Requirement already satisfied: rembg in /usr/local/lib/python3.10/dist-packages (2.0.61)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg) (4.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.26.4)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from rembg) (4.10.0.84)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from rembg) (11.0.0)\n","Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from rembg) (1.8.2)\n","Requirement already satisfied: pymatting in /usr/local/lib/python3.10/dist-packages (from rembg) (1.1.13)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from rembg) (0.19.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.14.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rembg) (4.64.1)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (24.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.22.3)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (4.3.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (24.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (2.32.3)\n","Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting->rembg) (0.60.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (3.4.2)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2.22.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2024.12.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (1.8.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting->rembg) (0.43.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2024.12.14)\n","Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Fetched 129 kB in 1s (95.3 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","50 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.22.0)\n","Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.7)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.64.1)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.12.14)\n","Requirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 1)) (0.2.0)\n","Requirement already satisfied: imageio==2.22.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 2)) (2.22.0)\n","Requirement already satisfied: imageio-ffmpeg==0.4.7 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 3)) (0.4.7)\n","Requirement already satisfied: matplotlib==3.6.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 4)) (3.6.0)\n","Collecting numpy==1.23.3 (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 5))\n","  Using cached numpy-1.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n","Requirement already satisfied: pandas==1.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 6)) (1.5.0)\n","Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 7)) (2.8.2)\n","Requirement already satisfied: pytz==2022.2.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 8)) (2022.2.1)\n","Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 9)) (6.0)\n","Requirement already satisfied: scikit-image==0.19.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 10)) (0.19.3)\n","Requirement already satisfied: scikit-learn==1.1.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 11)) (1.1.2)\n","Collecting scipy==1.9.1 (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 12))\n","  Using cached scipy-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n","Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 13)) (1.12.1)\n","Requirement already satisfied: torchvision==0.13.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 14)) (0.13.1)\n","Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 15)) (4.64.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python==0.2.0->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 1)) (1.0.0)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio==2.22.0->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 2)) (11.0.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.0->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.0->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 4)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.0->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 4)) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.0->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 4)) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.0->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 4)) (24.2)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.0->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 4)) (3.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 7)) (1.17.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 10)) (3.4.2)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 10)) (2024.12.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 10)) (1.8.0)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.2->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 11)) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.2->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 11)) (3.5.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 13)) (4.12.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 14)) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 14)) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 14)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 14)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1->-r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt (line 14)) (2024.12.14)\n","Using cached numpy-1.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","Using cached scipy-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n","Installing collected packages: numpy, scipy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.14.1\n","    Uninstalling scipy-1.14.1:\n","      Successfully uninstalled scipy-1.14.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.3 which is incompatible.\n","albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.3 which is incompatible.\n","albumentations 1.4.20 requires scipy>=1.10.0, but you have scipy 1.9.1 which is incompatible.\n","bigframes 1.29.0 requires matplotlib>=3.7.1, but you have matplotlib 3.6.0 which is incompatible.\n","bigframes 1.29.0 requires numpy>=1.24.0, but you have numpy 1.23.3 which is incompatible.\n","bigframes 1.29.0 requires pandas>=1.5.3, but you have pandas 1.5.0 which is incompatible.\n","bigframes 1.29.0 requires pytz>=2022.7, but you have pytz 2022.2.1 which is incompatible.\n","bigframes 1.29.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.2 which is incompatible.\n","chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.3 which is incompatible.\n","cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.0 which is incompatible.\n","ibis-framework 9.2.0 requires pandas<3,>=1.5.3, but you have pandas 1.5.0 which is incompatible.\n","ibis-framework 9.2.0 requires pytz>=2022.7, but you have pytz 2022.2.1 which is incompatible.\n","jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.3 which is incompatible.\n","jax 0.4.33 requires scipy>=1.10, but you have scipy 1.9.1 which is incompatible.\n","jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.3 which is incompatible.\n","jaxlib 0.4.33 requires scipy>=1.10, but you have scipy 1.9.1 which is incompatible.\n","mizani 0.13.1 requires numpy>=1.23.5, but you have numpy 1.23.3 which is incompatible.\n","mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.0 which is incompatible.\n","mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.1.2 which is incompatible.\n","pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.23.3 which is incompatible.\n","peft 0.14.0 requires torch>=1.13.0, but you have torch 1.12.1 which is incompatible.\n","plotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.6.0 which is incompatible.\n","plotnine 0.14.4 requires numpy>=1.23.5, but you have numpy 1.23.3 which is incompatible.\n","plotnine 0.14.4 requires pandas>=2.2.0, but you have pandas 1.5.0 which is incompatible.\n","pymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.23.3 which is incompatible.\n","tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.23.3 which is incompatible.\n","xarray 2024.11.0 requires numpy>=1.24, but you have numpy 1.23.3 which is incompatible.\n","xarray 2024.11.0 requires pandas>=2.1, but you have pandas 1.5.0 which is incompatible.\n","yfinance 0.2.50 requires pytz>=2022.5, but you have pytz 2022.2.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.23.3 scipy-1.9.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","scipy"]},"id":"27a9007db12848c585ee929c4f090a49"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sync-batchnorm in /usr/local/lib/python3.10/dist-packages (0.0.1)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.23.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Collecting numpy>=1.21.2 (from opencv-python-headless)\n","  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n","Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","Installing collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.3\n","    Uninstalling numpy-1.23.3:\n","      Successfully uninstalled numpy-1.23.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 1.4.20 requires scipy>=1.10.0, but you have scipy 1.9.1 which is incompatible.\n","bigframes 1.29.0 requires matplotlib>=3.7.1, but you have matplotlib 3.6.0 which is incompatible.\n","bigframes 1.29.0 requires pandas>=1.5.3, but you have pandas 1.5.0 which is incompatible.\n","bigframes 1.29.0 requires pytz>=2022.7, but you have pytz 2022.2.1 which is incompatible.\n","bigframes 1.29.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.2 which is incompatible.\n","cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.0 which is incompatible.\n","ibis-framework 9.2.0 requires pandas<3,>=1.5.3, but you have pandas 1.5.0 which is incompatible.\n","ibis-framework 9.2.0 requires pytz>=2022.7, but you have pytz 2022.2.1 which is incompatible.\n","jax 0.4.33 requires scipy>=1.10, but you have scipy 1.9.1 which is incompatible.\n","jaxlib 0.4.33 requires scipy>=1.10, but you have scipy 1.9.1 which is incompatible.\n","mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.0 which is incompatible.\n","mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.1.2 which is incompatible.\n","peft 0.14.0 requires torch>=1.13.0, but you have torch 1.12.1 which is incompatible.\n","plotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.6.0 which is incompatible.\n","plotnine 0.14.4 requires pandas>=2.2.0, but you have pandas 1.5.0 which is incompatible.\n","scipy 1.9.1 requires numpy<1.25.0,>=1.18.5, but you have numpy 1.26.4 which is incompatible.\n","xarray 2024.11.0 requires pandas>=2.1, but you have pandas 1.5.0 which is incompatible.\n","yfinance 0.2.50 requires pytz>=2022.5, but you have pytz 2022.2.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.4\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.22.0)\n","Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.7)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.64.1)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.12.14)\n","Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\n","Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.41.1)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n","Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.0)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n","Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2022.2.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.20)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n","Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.6.0)\n","Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n","Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n","Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n","Collecting scipy>=1.10 (from jax->mediapipe)\n","  Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n","Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.9.1\n","    Uninstalling scipy-1.9.1:\n","      Successfully uninstalled scipy-1.9.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n","mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.0 which is incompatible.\n","mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.1.2 which is incompatible.\n","plotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.6.0 which is incompatible.\n","plotnine 0.14.4 requires pandas>=2.2.0, but you have pandas 1.5.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed scipy-1.14.1\n"]}],"source":["# Install dependencies\n","from google.colab import files\n","\n","!pip install opencv-python-headless tensorflow\n","!pip install mediapipe opencv-python\n","!pip install gradio\n","!pip install rembg onnxruntime\n","# First, install the rembg library if you haven't already\n","!pip install rembg\n","!sudo apt update && sudo apt install ffmpeg\n","!pip install moviepy\n","\n","!pip install -r /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/requirements.txt\n","!pip install sync-batchnorm\n","!pip install opencv-python-headless tensorflow\n","\n","!pip install moviepy\n","!pip install ffmpeg-python\n","!pip install streamlit\n","!pip install mediapipe opencv-python\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"elapsed":5683,"status":"ok","timestamp":1733841869457,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"},"user_tz":-330},"id":"FmR2Io3cC_jE","outputId":"269095c5-2844-40cd-956a-764e891fd496"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://4792a7f9302e288ab4.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://4792a7f9302e288ab4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":5}],"source":["# Import Gradio\n","import gradio as gr\n","\n","# UI Layout\n","def create_ui():\n","    # Define UI elements\n","    with gr.Blocks(css=\".gradio-container {background-color: black; color: white;}\") as demo:\n","        # Title\n","        gr.Markdown(\"<h1 style='text-align: center; color: white;'>Generate Age-specific Animation using AI</h1>\")\n","\n","        # Display Image (Introductory image preview)\n","        gr.Image(value=\"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/websiteimagesample.png\", label=\"Animation Preview\", interactive=False)\n","\n","        # Upload Section\n","        gr.Markdown(\"<h2 style='text-align: center; color: white;'>Upload 3 Images!</h2>\")\n","\n","        # Image Upload Boxes with Preview\n","        with gr.Row():\n","            image1 = gr.Image(label=\"Image 1\", interactive=True, type=\"filepath\")\n","            image2 = gr.Image(label=\"Image 2\", interactive=True, type=\"filepath\")\n","            image3 = gr.Image(label=\"Image 3\", interactive=True, type=\"filepath\")\n","\n","        # Generate Video Button\n","        generate_button = gr.Button(\"Generate Video\")\n","\n","        # Playable Video Section\n","        output_video = gr.Video(label=\"Generated Video\")\n","\n","        # Button Click Action (demo only)\n","        def generate_video_fn(img1, img2, img3):\n","            # Placeholder action; replace with actual video generation function\n","            return \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/age_and_gender_detection/output_videos/generated_video.mp4\"\n","\n","        # Define interactions\n","        generate_button.click(generate_video_fn, [image1, image2, image3], output_video)\n","\n","    return demo\n","\n","# Launch the UI\n","create_ui().launch()\n"]},{"cell_type":"code","source":["# test  Age and Gender Detection Module\n","import cv2 as cv\n","\n","def getFaceBox(net, frame, conf_threshold=0.7):\n","    frameHeight = frame.shape[0]\n","    frameWidth = frame.shape[1]\n","    blob = cv.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], True, False)\n","\n","    net.setInput(blob)\n","    detections = net.forward()\n","    bboxes = []\n","    for i in range(detections.shape[2]):\n","        confidence = detections[0, 0, i, 2]\n","        if confidence > conf_threshold:\n","            x1 = int(detections[0, 0, i, 3] * frameWidth)\n","            y1 = int(detections[0, 0, i, 4] * frameHeight)\n","            x2 = int(detections[0, 0, i, 5] * frameWidth)\n","            y2 = int(detections[0, 0, i, 6] * frameHeight)\n","            bboxes.append([x1, y1, x2, y2])\n","    return bboxes\n","\n","faceProto = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/opencv_face_detector.pbtxt\"\n","faceModel = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/opencv_face_detector_uint8.pb\"\n","\n","ageProto = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/age_deploy.prototxt'\n","ageModel = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/age_net.caffemodel'\n","genderProto = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/gender_deploy.prototxt'\n","genderModel = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/gender_net.caffemodel'\n","\n","MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n","ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-55)', '(56-70)', '(71-100)']\n","genderList = ['Male', 'Female']\n","\n","# Load network\n","ageNet = cv.dnn.readNet(ageModel, ageProto)\n","genderNet = cv.dnn.readNet(genderModel, genderProto)\n","faceNet = cv.dnn.readNet(faceModel, faceProto)\n","\n","padding = 20\n","\n","def age_gender_detector(frame):\n","    bboxes = getFaceBox(faceNet, frame)\n","    ages = []\n","    for bbox in bboxes:\n","        face = frame[max(0, bbox[1]-padding):min(bbox[3]+padding, frame.shape[0]-1),\n","                     max(0, bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n","\n","        blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n","        genderNet.setInput(blob)\n","        genderPreds = genderNet.forward()\n","        ageNet.setInput(blob)\n","        agePreds = ageNet.forward()\n","\n","        # Get the detected age range\n","        age_range = ageList[agePreds[0].argmax()]  # Get the age category\n","\n","        # Convert age range to midpoint integer (used for the next module)\n","        age_min, age_max = map(int, age_range.strip('()').split('-'))\n","        age_mid = (age_min + age_max) // 2\n","        ages.append(age_mid)  # Store the midpoint integer\n","\n","    return ages  # Return midpoints for the next module\n","\n","# List of input image paths (change the paths to your images)\n","image_paths = [\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/images/111.png\",\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/images/222.png\",\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/images/333.png\",\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/images/444.png\",\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/images/555.png\",\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/images/666.png\"\n","]\n","\n","# Variables to store detected ages for each image\n","detected_ages = []  # To hold ages for all images\n","\n","for idx, image_path in enumerate(image_paths):\n","    input_image = cv.imread(image_path)\n","    ages = age_gender_detector(input_image)  # Get detected ages (midpoints) for the current image\n","    detected_ages.append(ages)  # Append to the detected_ages list\n","\n","# Output the detected age categories for the print statement (just categories, not exact ages)\n","for idx, age in enumerate(detected_ages):\n","    # Get the age categories for printing based on midpoint\n","    age_categories = []\n","    for mid_age in age:\n","        # Match the midpoint to its category\n","        for category in ageList:\n","            age_min, age_max = map(int, category.strip('()').split('-'))\n","            if age_min <= mid_age <= age_max:\n","                age_categories.append(category)\n","                break\n","\n","    print(f\"Detected Age Categories for Image {idx + 1}: {', '.join(age_categories)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqNx_1aCQWZi","executionInfo":{"status":"ok","timestamp":1734340090638,"user_tz":-330,"elapsed":25090,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"}},"outputId":"54ca7bf7-b8f7-4e7d-eec2-fadbb0b1a3c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected Age Categories for Image 1: (25-32)\n","Detected Age Categories for Image 2: (4-6)\n","Detected Age Categories for Image 3: (8-12)\n","Detected Age Categories for Image 4: (15-20)\n","Detected Age Categories for Image 5: (8-12)\n","Detected Age Categories for Image 6: (8-12)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEbOGL2I3mgQ"},"outputs":[],"source":["import cv2 as cv\n","import numpy as np\n","import imageio\n","from skimage import transform, img_as_ubyte\n","import yaml  # Make sure to import yaml if you're loading configurations\n","import torch  # Ensure torch is imported\n","import sys\n","from skimage.util import img_as_ubyte\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3404,"status":"ok","timestamp":1734340109906,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"},"user_tz":-330},"id":"5wb2AHF4-zDa","outputId":"dda2f886-33f3-4107-b612-5d622da4107e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Detected Age Categories for Image 1: (25-32)\n","Detected Age Categories for Image 2: (56-70)\n","Detected Age Categories for Image 3: (8-12)\n"]}],"source":["# Age and Gender Detection Module\n","import cv2 as cv\n","\n","def getFaceBox(net, frame, conf_threshold=0.7):\n","    frameHeight = frame.shape[0]\n","    frameWidth = frame.shape[1]\n","    blob = cv.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], True, False)\n","\n","    net.setInput(blob)\n","    detections = net.forward()\n","    bboxes = []\n","    for i in range(detections.shape[2]):\n","        confidence = detections[0, 0, i, 2]\n","        if confidence > conf_threshold:\n","            x1 = int(detections[0, 0, i, 3] * frameWidth)\n","            y1 = int(detections[0, 0, i, 4] * frameHeight)\n","            x2 = int(detections[0, 0, i, 5] * frameWidth)\n","            y2 = int(detections[0, 0, i, 6] * frameHeight)\n","            bboxes.append([x1, y1, x2, y2])\n","    return bboxes\n","\n","faceProto = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/opencv_face_detector.pbtxt\"\n","faceModel = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/opencv_face_detector_uint8.pb\"\n","\n","ageProto = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/age_deploy.prototxt'\n","ageModel = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/age_net.caffemodel'\n","genderProto = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/gender_deploy.prototxt'\n","genderModel = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/gender_net.caffemodel'\n","\n","MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n","ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-55)', '(56-70)', '(71-100)']\n","genderList = ['Male', 'Female']\n","\n","# Load network\n","ageNet = cv.dnn.readNet(ageModel, ageProto)\n","genderNet = cv.dnn.readNet(genderModel, genderProto)\n","faceNet = cv.dnn.readNet(faceModel, faceProto)\n","\n","padding = 20\n","\n","def age_gender_detector(frame):\n","    bboxes = getFaceBox(faceNet, frame)\n","    ages = []\n","    for bbox in bboxes:\n","        face = frame[max(0, bbox[1]-padding):min(bbox[3]+padding, frame.shape[0]-1),\n","                     max(0, bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n","\n","        blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n","        genderNet.setInput(blob)\n","        genderPreds = genderNet.forward()\n","        ageNet.setInput(blob)\n","        agePreds = ageNet.forward()\n","\n","        # Get the detected age range\n","        age_range = ageList[agePreds[0].argmax()]  # Get the age category\n","\n","        # Convert age range to midpoint integer (used for the next module)\n","        age_min, age_max = map(int, age_range.strip('()').split('-'))\n","        age_mid = (age_min + age_max) // 2\n","        ages.append(age_mid)  # Store the midpoint integer\n","\n","    return ages  # Return midpoints for the next module\n","\n","# List of input image paths (change the paths to your images)\n","image_paths = [\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/image1.png\",\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/image2.png\",\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/image3.png\"\n","]\n","\n","# Variables to store detected ages for each image\n","detected_ages = []  # To hold ages for all images\n","\n","for idx, image_path in enumerate(image_paths):\n","    input_image = cv.imread(image_path)\n","    ages = age_gender_detector(input_image)  # Get detected ages (midpoints) for the current image\n","    detected_ages.append(ages)  # Append to the detected_ages list\n","\n","# Output the detected age categories for the print statement (just categories, not exact ages)\n","for idx, age in enumerate(detected_ages):\n","    # Get the age categories for printing based on midpoint\n","    age_categories = []\n","    for mid_age in age:\n","        # Match the midpoint to its category\n","        for category in ageList:\n","            age_min, age_max = map(int, category.strip('()').split('-'))\n","            if age_min <= mid_age <= age_max:\n","                age_categories.append(category)\n","                break\n","\n","    print(f\"Detected Age Categories for Image {idx + 1}: {', '.join(age_categories)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"elapsed":1042,"status":"error","timestamp":1734340142247,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"},"user_tz":-330},"id":"zt_rLFFbyeLY","outputId":"3c8e4ec7-c10a-4cb2-89ca-819badd16cbe"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'demo'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-6c3cca525402>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbase64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mb64encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdemo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_checkpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_animation\u001b[0m  \u001b[0;31m# Ensure these functions are correctly implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimg_as_ubyte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtempfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'demo'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# First order motion model Module\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/project_files/age_and_gender_detection')\n","sys.path.append('/content/drive/MyDrive/project_files/age_and_gender_detection/modules')\n","\n","\n","import PIL.Image\n","import cv2\n","import ffmpeg\n","import imageio\n","import numpy as np\n","import os\n","import warnings\n","from base64 import b64encode\n","from demo import load_checkpoints, make_animation  # Ensure these functions are correctly implemented\n","from skimage import img_as_ubyte\n","from tempfile import NamedTemporaryFile\n","from tqdm.auto import tqdm\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Paths for images and age motion videos\n","image_paths = [\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/image1.png\",\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/image2.png\",\n","    \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/image3.png\"\n","]\n","\n","age_motion_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/agemotionvideos\"\n","output_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/output_videos\"\n","\n","# Ensure output directory exists\n","os.makedirs(output_videos_dir, exist_ok=True)\n","\n","# Function to get the driving video path based on the age\n","def get_driving_video_path(age):\n","    if age < 2:\n","        return os.path.join(age_motion_videos_dir, '0to2.mp4')\n","    elif 3 <= age < 6:\n","        return os.path.join(age_motion_videos_dir, '4to6.mp4')\n","    elif 7 <= age < 12:\n","        return os.path.join(age_motion_videos_dir, '8to12.mp4')\n","    elif 13 <= age < 20:\n","        return os.path.join(age_motion_videos_dir, '15to20.mp4')\n","    elif 21 <= age < 32:\n","        return os.path.join(age_motion_videos_dir, '25to32.mp4')\n","    elif 33 <= age < 43:\n","        return os.path.join(age_motion_videos_dir, '38to43.mp4')\n","    elif 44 <= age < 59:\n","        return os.path.join(age_motion_videos_dir, '48to53.mp4')\n","    elif 60 <= age < 100:\n","        return os.path.join(age_motion_videos_dir, '60to100.mp4')\n","\n","# Function to resize images to (256, 256)\n","def resize_image(image):\n","    return image.resize((256, 256), resample=PIL.Image.LANCZOS)\n","\n","# FOMM processing function\n","def fomm_process(image_path, driving_video_path, output_dir):\n","    # Load and resize the input image\n","    input_image = PIL.Image.open(image_path).convert(\"RGB\")\n","    input_image_resized = resize_image(input_image)\n","\n","    # Load the driving video\n","    reader = imageio.get_reader(driving_video_path, mode='I', format='FFMPEG')\n","    fps = reader.get_meta_data()['fps']\n","    driving_video = [frame for frame in reader]\n","\n","    # Load the FOMM model\n","    model_name = 'vox'  # Model name\n","    checkpoint_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/config/vox-256.yaml\"\n","    weights_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/checkpoints/vox-cpk.pth\"\n","\n","    generator, kp_detector = load_checkpoints(config_path=checkpoint_path, checkpoint_path=weights_path)\n","\n","    # Generate the animation\n","    predictions = make_animation(\n","        np.array(input_image_resized) / 255.0,  # Normalize the image\n","        [cv2.resize(frame, (256, 256)) / 255.0 for frame in driving_video],  # Normalize frames\n","        generator,\n","        kp_detector,\n","        relative=True,  # Adjust as necessary\n","        adapt_movement_scale=True  # Adjust as necessary\n","    )\n","\n","    # Save the output video\n","    output_video_path = os.path.join(output_dir, os.path.basename(image_path).split('.')[0] + '_output.mp4')\n","    imageio.mimsave(output_video_path, [img_as_ubyte(frame) for frame in predictions], fps=fps)\n","\n","    return output_video_path\n","\n","# Example usage\n","detected_ages = []  # This will hold the ages from the age detection module\n","\n","# Assuming this part of the code is executed after running the age detection module\n","for idx, image_path in enumerate(image_paths):\n","    input_image = cv.imread(image_path)\n","    ages = age_gender_detector(input_image)  # Get detected ages for the current image\n","    detected_ages.append(ages)  # Append to the detected_ages list\n","\n","output_videos = []\n","for image_path, ages in zip(image_paths, detected_ages):\n","    # Use the first detected age for driving video selection\n","    driving_video_path = get_driving_video_path(ages[0])  # Use the first age in the list\n","    output_video = fomm_process(image_path, driving_video_path, output_videos_dir)\n","    output_videos.append(output_video)\n","\n","# Output video paths\n","for idx, video in enumerate(output_videos):\n","    print(f\"Output video for image {idx + 1}: {video}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5566,"status":"ok","timestamp":1733918407490,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"},"user_tz":-330},"id":"DU9jRB24cugL","outputId":"d13eaa20-62ce-4ac1-bd8f-8532b535eb41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n","Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.18)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n","Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.6.0)\n","Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n","Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n","Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n","Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.14.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"]}],"source":["!pip install opencv-python-headless tensorflow\n","!pip install mediapipe opencv-python\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10535,"status":"ok","timestamp":1733918437780,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"},"user_tz":-330},"id":"CohVSloVTE5A","outputId":"4bc8561e-22e3-49e4-e3e7-095b3dd1898a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Background-removed video saved at /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos/image1_output_no_bg.mp4\n","Background-removed video saved at /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos/image2_output_no_bg.mp4\n","Background-removed video saved at /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos/image3_output_no_bg.mp4\n"]}],"source":["# Background removal Module\n","import cv2\n","import mediapipe as mp\n","import numpy as np\n","import os\n","\n","# Initialize MediaPipe variables\n","mp_selfie_segmentation = mp.solutions.selfie_segmentation\n","\n","# Paths\n","input_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/output_videos\"\n","bg_removed_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos\"\n","\n","# Ensure output directory exists\n","os.makedirs(bg_removed_videos_dir, exist_ok=True)\n","\n","# Function to remove background and save video with black background\n","def remove_background(input_video_path, output_video_path):\n","    cap = cv2.VideoCapture(input_video_path)\n","    if not cap.isOpened():\n","        print(f\"Error: Could not open video file {input_video_path}\")\n","        return\n","\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","    # Use MP4 codec\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height), isColor=True)\n","\n","    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as selfie_segmentation:\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            # Convert the frame to RGB\n","            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            results = selfie_segmentation.process(rgb_frame)\n","\n","            # Create the binary mask for segmentation\n","            condition = results.segmentation_mask > 0.5\n","\n","            # Apply the mask to set background pixels to black\n","            output_frame = np.zeros_like(frame)  # Start with a black background\n","            output_frame[condition] = frame[condition]  # Set person pixels from original frame\n","\n","            # Write the frame with the black background\n","            out.write(output_frame)\n","\n","    cap.release()\n","    out.release()\n","    print(f\"Background-removed video saved at {output_video_path}\")\n","\n","# Process each video in the input directory\n","for video_file in os.listdir(input_videos_dir):\n","    if video_file.endswith('.mp4'):\n","        input_video_path = os.path.join(input_videos_dir, video_file)\n","        output_video_path = os.path.join(bg_removed_videos_dir, video_file.replace('.mp4', '_no_bg.mp4'))\n","        remove_background(input_video_path, output_video_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5195,"status":"ok","timestamp":1733917746457,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"},"user_tz":-330},"id":"7_1WCV2hLF5N","outputId":"5f89c8c5-250b-4d6d-80ee-573bf0414b7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.22.0)\n","Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.7)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.64.1)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n"]}],"source":["!pip install moviepy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvEF7YvMk5tH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733918453572,"user_tz":-330,"elapsed":3662,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"}},"outputId":"360a6767-7734-4f9a-93e2-fa9c53186c3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Moviepy - Building video /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4.\n","Moviepy - Writing video /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\n","\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Moviepy - Done !\n","Moviepy - video ready /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\n","Concatenated video saved at: /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\n"]}],"source":["# video concatenation Module\n","from moviepy.editor import VideoFileClip, clips_array\n","import os\n","\n","# Paths\n","bg_removed_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos\"\n","merged_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\"\n","\n","# Get all background-removed video files\n","bg_removed_videos = sorted([os.path.join(bg_removed_videos_dir, f) for f in os.listdir(bg_removed_videos_dir) if f.endswith('.mp4')])\n","\n","# Load video clips\n","clips = [VideoFileClip(video).resize((256, 256)) for video in bg_removed_videos]\n","\n","# Concatenate clips horizontally\n","final_clip = clips_array([[clips[0], clips[1], clips[2]]])  # Adjust this if you have more or fewer clips\n","\n","# Write the output video\n","final_clip.write_videofile(merged_video_path, codec='libx264', fps=30)\n","\n","print(f\"Concatenated video saved at: {merged_video_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24331,"status":"ok","timestamp":1733917774424,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"},"user_tz":-330},"id":"ZMCS4ZsBISza","outputId":"b2b45ec3-9375-443c-a247-f4aec917e6cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rembg\n","  Downloading rembg-2.0.60-py3-none-any.whl.metadata (18 kB)\n","Collecting onnxruntime\n","  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg) (4.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.26.4)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from rembg) (4.10.0.84)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from rembg) (11.0.0)\n","Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from rembg) (1.8.2)\n","Collecting pymatting (from rembg)\n","  Downloading PyMatting-1.1.13-py3-none-any.whl.metadata (7.5 kB)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from rembg) (0.19.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.14.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rembg) (4.64.1)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (4.25.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.22.3)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (4.3.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (2.32.3)\n","Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting->rembg) (0.60.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (3.4.2)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2.22.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2024.9.20)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (1.8.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting->rembg) (0.43.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2024.8.30)\n","Downloading rembg-2.0.60-py3-none-any.whl (39 kB)\n","Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyMatting-1.1.13-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: humanfriendly, pymatting, coloredlogs, onnxruntime, rembg\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1 pymatting-1.1.13 rembg-2.0.60\n","Requirement already satisfied: rembg in /usr/local/lib/python3.10/dist-packages (2.0.60)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg) (4.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.26.4)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from rembg) (4.10.0.84)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from rembg) (11.0.0)\n","Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from rembg) (1.8.2)\n","Requirement already satisfied: pymatting in /usr/local/lib/python3.10/dist-packages (from rembg) (1.1.13)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from rembg) (0.19.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.14.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rembg) (4.64.1)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.22.3)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (4.3.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (24.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (2.32.3)\n","Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting->rembg) (0.60.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (3.4.2)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2.22.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2024.9.20)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (1.8.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting->rembg) (0.43.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2024.8.30)\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,194 kB]\n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,627 kB]\n","Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n","Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,537 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,515 kB]\n","Get:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [49.7 kB]\n","Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,458 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,475 kB]\n","Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,331 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,751 kB]\n","Fetched 27.6 MB in 4s (7,542 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","58 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 58 not upgraded.\n"]}],"source":["!pip install rembg onnxruntime\n","# First, install the rembg library if you haven't already\n","!pip install rembg\n","!sudo apt update && sudo apt install ffmpeg\n","!pip install moviepy\n"]},{"cell_type":"code","source":["import cv2\n","import os\n","import numpy as np\n","import shutil\n","\n","# Define paths\n","merged_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\"\n","merged_video_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_videoframes/\"\n","background_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/backgroundvideo.mp4\"\n","background_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/framesbackgroundvideo/\"\n","output_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/Untitled Folder/\"\n","final_output_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/final_outputvideo/final_output_video.mp4\"\n","\n","# Utility: Clear frames from a folder\n","def clear_folder(folder_path):\n","    if os.path.exists(folder_path):\n","        shutil.rmtree(folder_path)\n","    os.makedirs(folder_path)\n","\n","# Clear folders before processing\n","clear_folder(merged_video_frames_path)\n","clear_folder(background_frames_path)\n","clear_folder(output_frames_path)\n","\n","# Step 1: Extract frames from a video\n","def extract_frames_from_video(video_path, output_frames_path):\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = 0\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame_filename = os.path.join(output_frames_path, f'frame_{frame_count:04d}.png')\n","        cv2.imwrite(frame_filename, frame)\n","        frame_count += 1\n","    cap.release()\n","    print(f\"Extracted {frame_count} frames from {video_path}.\")\n","    return frame_count\n","\n","# Step 2: Replace black background with transparency\n","def replace_black_background_with_transparency(frames_path):\n","    frame_files = sorted(os.listdir(frames_path))\n","    for frame_file in frame_files:\n","        frame_path = os.path.join(frames_path, frame_file)\n","        frame = cv2.imread(frame_path)\n","        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","        lower_black = np.array([0, 0, 0])\n","        upper_black = np.array([180, 255, 30])\n","        mask = cv2.inRange(hsv, lower_black, upper_black)\n","        frame_bgra = cv2.cvtColor(frame, cv2.COLOR_BGR2BGRA)\n","        frame_bgra[mask != 0] = [0, 0, 0, 0]\n","        cv2.imwrite(frame_path, frame_bgra)\n","\n","# Step 3: Overlay transparent frames onto background frames\n","def overlay_frames(transparent_frames_path, background_frames_path, output_frames_path):\n","    transparent_frame_files = sorted(os.listdir(transparent_frames_path))\n","    background_frame_files = sorted(os.listdir(background_frames_path))\n","\n","    # Align frame counts by trimming extra frames from the longer video\n","    min_frame_count = min(len(transparent_frame_files), len(background_frame_files))\n","    transparent_frame_files = transparent_frame_files[:min_frame_count]\n","    background_frame_files = background_frame_files[:min_frame_count]\n","\n","    for i, frame_file in enumerate(transparent_frame_files):\n","        transparent_frame = cv2.imread(os.path.join(transparent_frames_path, frame_file), cv2.IMREAD_UNCHANGED)\n","        background_frame = cv2.imread(os.path.join(background_frames_path, background_frame_files[i]))\n","\n","        if transparent_frame.shape[2] == 4:  # Check for alpha channel\n","            alpha_channel = transparent_frame[:, :, 3] / 255.0\n","            for c in range(3):  # Blend RGB channels\n","                background_frame[:, :, c] = background_frame[:, :, c] * (1 - alpha_channel) + transparent_frame[:, :, c] * alpha_channel\n","\n","        output_frame_path = os.path.join(output_frames_path, f'output_frame_{i:04d}.png')\n","        cv2.imwrite(output_frame_path, background_frame)\n","\n","    print(f\"Overlay complete. {min_frame_count} frames processed.\")\n","\n","# Step 4: Combine frames into a video\n","def create_video_from_frames(frames_path, output_video_path, fps=30):\n","    frame_files = sorted(os.listdir(frames_path))\n","    if not frame_files:\n","        raise ValueError(\"No frames found to create a video.\")\n","\n","    first_frame = cv2.imread(os.path.join(frames_path, frame_files[0]))\n","    height, width, _ = first_frame.shape\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n","\n","    for frame_file in frame_files:\n","        frame = cv2.imread(os.path.join(frames_path, frame_file))\n","        video_writer.write(frame)\n","\n","    video_writer.release()\n","    print(f\"Video saved at {output_video_path}.\")\n","\n","# Execute the pipeline\n","merged_frame_count = extract_frames_from_video(merged_video_path, merged_video_frames_path)\n","replace_black_background_with_transparency(merged_video_frames_path)\n","background_frame_count = extract_frames_from_video(background_video_path, background_frames_path)\n","overlay_frames(merged_video_frames_path, background_frames_path, output_frames_path)\n","create_video_from_frames(output_frames_path, final_output_video_path, fps=30)\n"],"metadata":{"id":"8tuBVlTyXBZv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733918487854,"user_tz":-330,"elapsed":29579,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"}},"outputId":"9cb6dbf3-226f-442b-ee96-2f9d4401931e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted 213 frames from /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4.\n","Extracted 211 frames from /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/backgroundvideo.mp4.\n","Overlay complete. 211 frames processed.\n","Video saved at /content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/final_outputvideo/final_output_video.mp4.\n"]}]},{"cell_type":"code","source":["#final without age ui\n","import cv2 as cv\n","import sys\n","sys.path.append('/content/drive/MyDrive/project_files/age_and_gender_detection')\n","sys.path.append('/content/drive/MyDrive/project_files/age_and_gender_detection/modules')\n","import PIL.Image\n","import cv2\n","import ffmpeg\n","import imageio\n","import numpy as np\n","import os\n","import warnings\n","from base64 import b64encode\n","from demo import load_checkpoints, make_animation  # Ensure these functions are correctly implemented\n","from skimage import img_as_ubyte\n","from tempfile import NamedTemporaryFile\n","from tqdm.auto import tqdm\n","import cv2\n","import mediapipe as mp\n","import numpy as np\n","import os\n","import cv2\n","import os\n","import numpy as np\n","import shutil\n","from moviepy.editor import VideoFileClip, clips_array\n","import os\n","import gradio as gr\n","\n","def agespecificmotion(img1, img2, img3):\n","    \"\"\"\n","    A single function to handle the entire pipeline for generating an age-specific animation.\n","    Takes three uploaded image paths as input.\n","    \"\"\"\n","    # Save uploaded images to specific paths\n","    saved_paths = [\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image1.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image2.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image3.png\"\n","    ]\n","    # Save images to predefined paths\n","    for img, path in zip([img1, img2, img3], saved_paths):\n","        import shutil\n","        shutil.copy(img, path)\n","\n","    # Place all the pipeline code (modules/functions) here\n","\n","\n","\n","    # Age and Gender Detection Module\n","    def getFaceBox(net, frame, conf_threshold=0.7):\n","        frameHeight = frame.shape[0]\n","        frameWidth = frame.shape[1]\n","        blob = cv.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], True, False)\n","\n","        net.setInput(blob)\n","        detections = net.forward()\n","        bboxes = []\n","        for i in range(detections.shape[2]):\n","            confidence = detections[0, 0, i, 2]\n","            if confidence > conf_threshold:\n","                x1 = int(detections[0, 0, i, 3] * frameWidth)\n","                y1 = int(detections[0, 0, i, 4] * frameHeight)\n","                x2 = int(detections[0, 0, i, 5] * frameWidth)\n","                y2 = int(detections[0, 0, i, 6] * frameHeight)\n","                bboxes.append([x1, y1, x2, y2])\n","        return bboxes\n","\n","    faceProto = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/opencv_face_detector.pbtxt\"\n","    faceModel = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/opencv_face_detector_uint8.pb\"\n","\n","    ageProto = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/age_deploy.prototxt'\n","    ageModel = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/age_net.caffemodel'\n","    genderProto = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/gender_deploy.prototxt'\n","    genderModel = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/gender_net.caffemodel'\n","\n","    MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n","    ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-55)', '(56-70)', '(71-100)']\n","    genderList = ['Male', 'Female']\n","\n","    # Load network\n","    ageNet = cv.dnn.readNet(ageModel, ageProto)\n","    genderNet = cv.dnn.readNet(genderModel, genderProto)\n","    faceNet = cv.dnn.readNet(faceModel, faceProto)\n","\n","    padding = 20\n","\n","    def age_gender_detector(frame):\n","        bboxes = getFaceBox(faceNet, frame)\n","        ages = []\n","        for bbox in bboxes:\n","            face = frame[max(0, bbox[1]-padding):min(bbox[3]+padding, frame.shape[0]-1),\n","                        max(0, bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n","\n","            blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n","            genderNet.setInput(blob)\n","            genderPreds = genderNet.forward()\n","            ageNet.setInput(blob)\n","            agePreds = ageNet.forward()\n","\n","            # Get the detected age range\n","            age_range = ageList[agePreds[0].argmax()]  # Get the age category\n","\n","            # Convert age range to midpoint integer (used for the next module)\n","            age_min, age_max = map(int, age_range.strip('()').split('-'))\n","            age_mid = (age_min + age_max) // 2\n","            ages.append(age_mid)  # Store the midpoint integer\n","\n","        return ages  # Return midpoints for the next module\n","\n","    # List of input image paths (change the paths to your images)\n","    image_paths = [\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image1.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image2.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image3.png\"\n","    ]\n","\n","    # Variables to store detected ages for each image\n","    detected_ages = []  # To hold ages for all images\n","\n","    for idx, image_path in enumerate(image_paths):\n","        input_image = cv.imread(image_path)\n","        ages = age_gender_detector(input_image)  # Get detected ages (midpoints) for the current image\n","        detected_ages.append(ages)  # Append to the detected_ages list\n","\n","    # Output the detected age categories for the print statement (just categories, not exact ages)\n","    for idx, age in enumerate(detected_ages):\n","        # Get the age categories for printing based on midpoint\n","        age_categories = []\n","        for mid_age in age:\n","            # Match the midpoint to its category\n","            for category in ageList:\n","                age_min, age_max = map(int, category.strip('()').split('-'))\n","                if age_min <= mid_age <= age_max:\n","                    age_categories.append(category)\n","                    break\n","\n","        print(f\"Detected Age Categories for Image {idx + 1}: {', '.join(age_categories)}\")\n","\n","    # First order motion model Module\n","\n","\n","    warnings.filterwarnings(\"ignore\")\n","\n","    # Paths for images and age motion videos\n","    image_paths = [\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image1.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image2.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image3.png\"\n","    ]\n","\n","    age_motion_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/agemotionvideos\"\n","    output_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/output_videos\"\n","\n","    # Ensure output directory exists\n","    os.makedirs(output_videos_dir, exist_ok=True)\n","\n","    # Function to get the driving video path based on the age\n","    def get_driving_video_path(age):\n","        if age < 2:\n","            return os.path.join(age_motion_videos_dir, '0to2.mp4')\n","        elif 3 <= age < 6:\n","            return os.path.join(age_motion_videos_dir, '4to6.mp4')\n","        elif 7 <= age < 12:\n","            return os.path.join(age_motion_videos_dir, '8to12.mp4')\n","        elif 13 <= age < 20:\n","            return os.path.join(age_motion_videos_dir, '15to20.mp4')\n","        elif 21 <= age < 32:\n","            return os.path.join(age_motion_videos_dir, '25to32.mp4')\n","        elif 33 <= age < 43:\n","            return os.path.join(age_motion_videos_dir, '38to43.mp4')\n","        elif 44 <= age < 59:\n","            return os.path.join(age_motion_videos_dir, '48to53.mp4')\n","        elif 60 <= age < 100:\n","            return os.path.join(age_motion_videos_dir, '60to100.mp4')\n","\n","    # Function to resize images to (256, 256)\n","    def resize_image(image):\n","        return image.resize((256, 256), resample=PIL.Image.LANCZOS)\n","\n","    # FOMM processing function\n","    def fomm_process(image_path, driving_video_path, output_dir):\n","        # Load and resize the input image\n","        input_image = PIL.Image.open(image_path).convert(\"RGB\")\n","        input_image_resized = resize_image(input_image)\n","\n","        # Load the driving video\n","        reader = imageio.get_reader(driving_video_path, mode='I', format='FFMPEG')\n","        fps = reader.get_meta_data()['fps']\n","        driving_video = [frame for frame in reader]\n","\n","        # Load the FOMM model\n","        model_name = 'vox'  # Model name\n","        checkpoint_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/config/vox-256.yaml\"\n","        weights_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/checkpoints/vox-cpk.pth\"\n","\n","        generator, kp_detector = load_checkpoints(config_path=checkpoint_path, checkpoint_path=weights_path)\n","\n","        # Generate the animation\n","        predictions = make_animation(\n","            np.array(input_image_resized) / 255.0,  # Normalize the image\n","            [cv2.resize(frame, (256, 256)) / 255.0 for frame in driving_video],  # Normalize frames\n","            generator,\n","            kp_detector,\n","            relative=True,  # Adjust as necessary\n","            adapt_movement_scale=True  # Adjust as necessary\n","        )\n","\n","        # Save the output video\n","        output_video_path = os.path.join(output_dir, os.path.basename(image_path).split('.')[0] + '_output.mp4')\n","        imageio.mimsave(output_video_path, [img_as_ubyte(frame) for frame in predictions], fps=fps)\n","\n","        return output_video_path\n","\n","    # Example usage\n","    detected_ages = []  # This will hold the ages from the age detection module\n","\n","    # Assuming this part of the code is executed after running the age detection module\n","    for idx, image_path in enumerate(image_paths):\n","        input_image = cv.imread(image_path)\n","        ages = age_gender_detector(input_image)  # Get detected ages for the current image\n","        detected_ages.append(ages)  # Append to the detected_ages list\n","\n","    output_videos = []\n","    for image_path, ages in zip(image_paths, detected_ages):\n","        # Use the first detected age for driving video selection\n","        driving_video_path = get_driving_video_path(ages[0])  # Use the first age in the list\n","        output_video = fomm_process(image_path, driving_video_path, output_videos_dir)\n","        output_videos.append(output_video)\n","\n","    # Output video paths\n","    for idx, video in enumerate(output_videos):\n","        print(f\"Output video for image {idx + 1}: {video}\")\n","\n","\n","\n","    # Background removal Module\n","\n","    # Initialize MediaPipe variables\n","    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n","\n","    # Paths\n","    input_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/output_videos\"\n","    bg_removed_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos\"\n","\n","    # Ensure output directory exists\n","    os.makedirs(bg_removed_videos_dir, exist_ok=True)\n","\n","    # Function to remove background and save video with black background\n","    def remove_background(input_video_path, output_video_path):\n","        cap = cv2.VideoCapture(input_video_path)\n","        if not cap.isOpened():\n","            print(f\"Error: Could not open video file {input_video_path}\")\n","            return\n","\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","        # Use MP4 codec\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height), isColor=True)\n","\n","        with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as selfie_segmentation:\n","            while cap.isOpened():\n","                ret, frame = cap.read()\n","                if not ret:\n","                    break\n","\n","                # Convert the frame to RGB\n","                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                results = selfie_segmentation.process(rgb_frame)\n","\n","                # Create the binary mask for segmentation\n","                condition = results.segmentation_mask > 0.5\n","\n","                # Apply the mask to set background pixels to black\n","                output_frame = np.zeros_like(frame)  # Start with a black background\n","                output_frame[condition] = frame[condition]  # Set person pixels from original frame\n","\n","                # Write the frame with the black background\n","                out.write(output_frame)\n","\n","        cap.release()\n","        out.release()\n","        print(f\"Background-removed video saved at {output_video_path}\")\n","\n","    # Process each video in the input directory\n","    for video_file in os.listdir(input_videos_dir):\n","        if video_file.endswith('.mp4'):\n","            input_video_path = os.path.join(input_videos_dir, video_file)\n","            output_video_path = os.path.join(bg_removed_videos_dir, video_file.replace('.mp4', '_no_bg.mp4'))\n","            remove_background(input_video_path, output_video_path)\n","\n","    # video concatenation Module\n","\n","    # Paths\n","    bg_removed_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos\"\n","    merged_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\"\n","\n","    # Get all background-removed video files\n","    bg_removed_videos = sorted([os.path.join(bg_removed_videos_dir, f) for f in os.listdir(bg_removed_videos_dir) if f.endswith('.mp4')])\n","\n","    # Load video clips\n","    clips = [VideoFileClip(video).resize((256, 256)) for video in bg_removed_videos]\n","\n","    # Concatenate clips horizontally\n","    final_clip = clips_array([[clips[0], clips[1], clips[2]]])  # Adjust this if you have more or fewer clips\n","\n","    # Write the output video\n","    final_clip.write_videofile(merged_video_path, codec='libx264', fps=30)\n","\n","    print(f\"Concatenated video saved at: {merged_video_path}\")\n","\n","    # Define paths\n","    merged_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\"\n","    merged_video_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_videoframes/\"\n","    background_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/backgroundvideo.mp4\"\n","    background_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/framesbackgroundvideo/\"\n","    output_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/Untitled Folder/\"\n","    final_output_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/final_outputvideo/final_output_video.mp4\"\n","\n","    # Utility: Clear frames from a folder\n","    def clear_folder(folder_path):\n","        if os.path.exists(folder_path):\n","            shutil.rmtree(folder_path)\n","        os.makedirs(folder_path)\n","\n","    # Clear folders before processing\n","    clear_folder(merged_video_frames_path)\n","    clear_folder(background_frames_path)\n","    clear_folder(output_frames_path)\n","\n","    # Step 1: Extract frames from a video\n","    def extract_frames_from_video(video_path, output_frames_path):\n","        cap = cv2.VideoCapture(video_path)\n","        frame_count = 0\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame_filename = os.path.join(output_frames_path, f'frame_{frame_count:04d}.png')\n","            cv2.imwrite(frame_filename, frame)\n","            frame_count += 1\n","        cap.release()\n","        print(f\"Extracted {frame_count} frames from {video_path}.\")\n","        return frame_count\n","\n","    # Step 2: Replace black background with transparency\n","    def replace_black_background_with_transparency(frames_path):\n","        frame_files = sorted(os.listdir(frames_path))\n","        for frame_file in frame_files:\n","            frame_path = os.path.join(frames_path, frame_file)\n","            frame = cv2.imread(frame_path)\n","            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","            lower_black = np.array([0, 0, 0])\n","            upper_black = np.array([180, 255, 30])\n","            mask = cv2.inRange(hsv, lower_black, upper_black)\n","            frame_bgra = cv2.cvtColor(frame, cv2.COLOR_BGR2BGRA)\n","            frame_bgra[mask != 0] = [0, 0, 0, 0]\n","            cv2.imwrite(frame_path, frame_bgra)\n","\n","    # Step 3: Overlay transparent frames onto background frames\n","    def overlay_frames(transparent_frames_path, background_frames_path, output_frames_path):\n","        transparent_frame_files = sorted(os.listdir(transparent_frames_path))\n","        background_frame_files = sorted(os.listdir(background_frames_path))\n","\n","        # Align frame counts by trimming extra frames from the longer video\n","        min_frame_count = min(len(transparent_frame_files), len(background_frame_files))\n","        transparent_frame_files = transparent_frame_files[:min_frame_count]\n","        background_frame_files = background_frame_files[:min_frame_count]\n","\n","        for i, frame_file in enumerate(transparent_frame_files):\n","            transparent_frame = cv2.imread(os.path.join(transparent_frames_path, frame_file), cv2.IMREAD_UNCHANGED)\n","            background_frame = cv2.imread(os.path.join(background_frames_path, background_frame_files[i]))\n","\n","            if transparent_frame.shape[2] == 4:  # Check for alpha channel\n","                alpha_channel = transparent_frame[:, :, 3] / 255.0\n","                for c in range(3):  # Blend RGB channels\n","                    background_frame[:, :, c] = background_frame[:, :, c] * (1 - alpha_channel) + transparent_frame[:, :, c] * alpha_channel\n","\n","            output_frame_path = os.path.join(output_frames_path, f'output_frame_{i:04d}.png')\n","            cv2.imwrite(output_frame_path, background_frame)\n","\n","        print(f\"Overlay complete. {min_frame_count} frames processed.\")\n","\n","    # Step 4: Combine frames into a video\n","    def create_video_from_frames(frames_path, output_video_path, fps=30):\n","        frame_files = sorted(os.listdir(frames_path))\n","        if not frame_files:\n","            raise ValueError(\"No frames found to create a video.\")\n","\n","        first_frame = cv2.imread(os.path.join(frames_path, frame_files[0]))\n","        height, width, _ = first_frame.shape\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n","\n","        for frame_file in frame_files:\n","            frame = cv2.imread(os.path.join(frames_path, frame_file))\n","            video_writer.write(frame)\n","\n","        video_writer.release()\n","        print(f\"Video saved at {output_video_path}.\")\n","\n","    # Execute the pipeline\n","    merged_frame_count = extract_frames_from_video(merged_video_path, merged_video_frames_path)\n","    replace_black_background_with_transparency(merged_video_frames_path)\n","    background_frame_count = extract_frames_from_video(background_video_path, background_frames_path)\n","    overlay_frames(merged_video_frames_path, background_frames_path, output_frames_path)\n","    create_video_from_frames(output_frames_path, final_output_video_path, fps=30)\n","\n","\n","    return \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/final_outputvideo/final_output_video.mp4\"\n","\n","\n","# UI Layout\n","def create_ui():\n","    # Define UI elements\n","    with gr.Blocks(css=\".gradio-container {background-color: black; color: white;}\") as demo:\n","        # Title\n","        gr.Markdown(\"<h1 style='text-align: center; color: white;'>Generate Age-specific Animation using AI</h1>\")\n","\n","        # Display Image (Introductory image preview)\n","        gr.Image(\n","            value=\"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/websiteimagesample.png\",\n","            label=\"Animation Preview\",\n","            interactive=False\n","        )\n","\n","        # Upload Section\n","        gr.Markdown(\"<h2 style='text-align: center; color: white;'>Upload 3 Images!</h2>\")\n","\n","        # Image Upload Boxes with Preview\n","        with gr.Row():\n","            image1 = gr.Image(label=\"Image 1\", interactive=True, type=\"filepath\")\n","            image2 = gr.Image(label=\"Image 2\", interactive=True, type=\"filepath\")\n","            image3 = gr.Image(label=\"Image 3\", interactive=True, type=\"filepath\")\n","\n","        # Generate Video Button\n","        generate_button = gr.Button(\"Generate Video\")\n","\n","        # Playable Video Section\n","        output_video = gr.Video(label=\"Generated Video\")\n","\n","        # Button Click Action\n","        generate_button.click(\n","            fn=agespecificmotion,  # Call the external function\n","            inputs=[image1, image2, image3],  # Pass inputs to the function\n","            outputs=output_video  # Display the returned video path\n","        )\n","\n","    return demo\n","\n","# Launch the UI\n","create_ui().launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"Ri1SrI09k8UE","executionInfo":{"status":"ok","timestamp":1734270510985,"user_tz":-330,"elapsed":3503,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"}},"outputId":"28193736-7be1-49ba-e4e9-f2d005ab858f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://3dd558588012fc825e.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://3dd558588012fc825e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["#final with enhancement\n","import cv2 as cv\n","import sys\n","sys.path.append('/content/drive/MyDrive/project_files/age_and_gender_detection')\n","sys.path.append('/content/drive/MyDrive/project_files/age_and_gender_detection/modules')\n","import PIL.Image\n","import cv2\n","import ffmpeg\n","import imageio\n","import numpy as np\n","import os\n","import warnings\n","from base64 import b64encode\n","from demo import load_checkpoints, make_animation  # Ensure these functions are correctly implemented\n","from skimage import img_as_ubyte\n","from tempfile import NamedTemporaryFile\n","from tqdm.auto import tqdm\n","import cv2\n","import mediapipe as mp\n","import numpy as np\n","import os\n","import cv2\n","import os\n","import numpy as np\n","import shutil\n","from moviepy.editor import VideoFileClip, clips_array\n","import os\n","import gradio as gr\n","\n","def agespecificmotion(img1, img2, img3):\n","    \"\"\"\n","    A single function to handle the entire pipeline for generating an age-specific animation.\n","    Takes three uploaded image paths as input.\n","    \"\"\"\n","    # Save uploaded images to specific paths\n","    saved_paths = [\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image1.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image2.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image3.png\"\n","    ]\n","    # Save images to predefined paths\n","    for img, path in zip([img1, img2, img3], saved_paths):\n","        import shutil\n","        shutil.copy(img, path)\n","\n","    # Place all the pipeline code (modules/functions) here\n","\n","\n","\n","    # Age and Gender Detection Module\n","    def getFaceBox(net, frame, conf_threshold=0.7):\n","        frameHeight = frame.shape[0]\n","        frameWidth = frame.shape[1]\n","        blob = cv.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], True, False)\n","\n","        net.setInput(blob)\n","        detections = net.forward()\n","        bboxes = []\n","        for i in range(detections.shape[2]):\n","            confidence = detections[0, 0, i, 2]\n","            if confidence > conf_threshold:\n","                x1 = int(detections[0, 0, i, 3] * frameWidth)\n","                y1 = int(detections[0, 0, i, 4] * frameHeight)\n","                x2 = int(detections[0, 0, i, 5] * frameWidth)\n","                y2 = int(detections[0, 0, i, 6] * frameHeight)\n","                bboxes.append([x1, y1, x2, y2])\n","        return bboxes\n","\n","    faceProto = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/opencv_face_detector.pbtxt\"\n","    faceModel = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/opencv_face_detector_uint8.pb\"\n","\n","    ageProto = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/age_deploy.prototxt'\n","    ageModel = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/age_net.caffemodel'\n","    genderProto = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/gender_deploy.prototxt'\n","    genderModel = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/gender_net.caffemodel'\n","\n","    MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n","    ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-55)', '(56-70)', '(71-100)']\n","    genderList = ['Male', 'Female']\n","\n","    # Load network\n","    ageNet = cv.dnn.readNet(ageModel, ageProto)\n","    genderNet = cv.dnn.readNet(genderModel, genderProto)\n","    faceNet = cv.dnn.readNet(faceModel, faceProto)\n","\n","    padding = 20\n","\n","    def age_gender_detector(frame):\n","        bboxes = getFaceBox(faceNet, frame)\n","        ages = []\n","        for bbox in bboxes:\n","            face = frame[max(0, bbox[1]-padding):min(bbox[3]+padding, frame.shape[0]-1),\n","                        max(0, bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n","\n","            blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n","            genderNet.setInput(blob)\n","            genderPreds = genderNet.forward()\n","            ageNet.setInput(blob)\n","            agePreds = ageNet.forward()\n","\n","            # Get the detected age range\n","            age_range = ageList[agePreds[0].argmax()]  # Get the age category\n","\n","            # Convert age range to midpoint integer (used for the next module)\n","            age_min, age_max = map(int, age_range.strip('()').split('-'))\n","            age_mid = (age_min + age_max) // 2\n","            ages.append(age_mid)  # Store the midpoint integer\n","\n","        return ages  # Return midpoints for the next module\n","\n","    # List of input image paths (change the paths to your images)\n","    image_paths = [\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image1.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image2.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image3.png\"\n","    ]\n","\n","    # Variables to store detected ages for each image\n","    detected_ages = []  # To hold ages for all images\n","\n","    for idx, image_path in enumerate(image_paths):\n","        input_image = cv.imread(image_path)\n","        ages = age_gender_detector(input_image)  # Get detected ages (midpoints) for the current image\n","        detected_ages.append(ages)  # Append to the detected_ages list\n","\n","    # Output the detected age categories for the print statement (just categories, not exact ages)\n","    for idx, age in enumerate(detected_ages):\n","        # Get the age categories for printing based on midpoint\n","        age_categories = []\n","        for mid_age in age:\n","            # Match the midpoint to its category\n","            for category in ageList:\n","                age_min, age_max = map(int, category.strip('()').split('-'))\n","                if age_min <= mid_age <= age_max:\n","                    age_categories.append(category)\n","                    break\n","\n","        print(f\"Detected Age Categories for Image {idx + 1}: {', '.join(age_categories)}\")\n","    age_message = (f\"The person in the first picture is {detected_ages[0][0]} years old, \"\n","                   f\"the person in the second picture is {detected_ages[1][0]} years old, \"\n","                   f\"and the person in the third picture is {detected_ages[2][0]} years old.\")\n","\n","    # First order motion model Module\n","\n","    warnings.filterwarnings(\"ignore\")\n","\n","    # Paths for images and age motion videos\n","    image_paths = [\n","      \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image1.png\",\n","      \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image2.png\",\n","      \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image3.png\"\n","    ]\n","\n","    age_motion_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/agemotionvideos\"\n","    output_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/output_videos\"\n","\n","    # Ensure output directory exists\n","    os.makedirs(output_videos_dir, exist_ok=True)\n","\n","    # Function to get the driving video path based on the age, alternating videos if multiple inputs fall in the same category\n","    category_counter = {\n","      '7-12': 0,\n","      '21-37': 0,\n","      '13-20':0\n","    }\n","\n","    def get_driving_video_path(age):\n","      if age < 2:\n","          return os.path.join(age_motion_videos_dir, '0to2.mp4')\n","      elif 3 <= age < 6:\n","          return os.path.join(age_motion_videos_dir, '4to6.mp4')\n","      elif 7 <= age < 12:\n","          video_file = f'8to12v{category_counter[\"7-12\"] % 2 + 1}.mp4'\n","          category_counter['7-12'] += 1\n","          return os.path.join(age_motion_videos_dir, video_file)\n","      elif 13 <= age < 20:\n","          video_file = f'13to20v{category_counter[\"13-20\"] % 2 + 1}.mp4'\n","          category_counter['13-20'] += 1\n","          return os.path.join(age_motion_videos_dir, video_file)\n","\n","      elif 21 <= age < 37:\n","          video_file = f'21to37v{category_counter[\"21-37\"] % 2 + 1}.mp4'\n","          category_counter['21-37'] += 1\n","          return os.path.join(age_motion_videos_dir, video_file)\n","      elif 38 <= age < 55:\n","          return os.path.join(age_motion_videos_dir, '38to55.mp4')\n","      elif 56 <= age < 70:\n","          return os.path.join(age_motion_videos_dir, '56to70.mp4')\n","      elif 71 <= age < 100:\n","          return os.path.join(age_motion_videos_dir, '71to100.mp4')\n","\n","    # Function to resize images to (256, 256)\n","    def resize_image(image):\n","      return image.resize((256, 256), resample=PIL.Image.LANCZOS)\n","\n","    # FOMM processing function\n","    def fomm_process(image_path, driving_video_path, output_dir):\n","      # Load and resize the input image\n","      input_image = PIL.Image.open(image_path).convert(\"RGB\")\n","      input_image_resized = resize_image(input_image)\n","\n","      # Load the driving video\n","      reader = imageio.get_reader(driving_video_path, mode='I', format='FFMPEG')\n","      fps = reader.get_meta_data()['fps']\n","      driving_video = [frame for frame in reader]\n","\n","      # Load the FOMM model\n","      model_name = 'vox'  # Model name\n","      checkpoint_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/config/vox-256.yaml\"\n","      weights_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/checkpoints/vox-cpk.pth\"\n","\n","      generator, kp_detector = load_checkpoints(config_path=checkpoint_path, checkpoint_path=weights_path)\n","\n","      # Generate the animation\n","      predictions = make_animation(\n","          np.array(input_image_resized) / 255.0,  # Normalize the image\n","          [cv2.resize(frame, (256, 256)) / 255.0 for frame in driving_video],  # Normalize frames\n","          generator,\n","          kp_detector,\n","          relative=True,  # Adjust as necessary\n","          adapt_movement_scale=True  # Adjust as necessary\n","      )\n","\n","      # Save the output video\n","      output_video_path = os.path.join(output_dir, os.path.basename(image_path).split('.')[0] + '_output.mp4')\n","      imageio.mimsave(output_video_path, [img_as_ubyte(frame) for frame in predictions], fps=fps)\n","\n","      return output_video_path\n","\n","    # Example usage\n","    detected_ages = []  # This will hold the ages from the age detection module\n","\n","    # Assuming this part of the code is executed after running the age detection module\n","    for idx, image_path in enumerate(image_paths):\n","      input_image = cv.imread(image_path)\n","      ages = age_gender_detector(input_image)  # Get detected ages for the current image\n","      detected_ages.append(ages)  # Append to the detected_ages list\n","\n","    output_videos = []\n","    for image_path, ages in zip(image_paths, detected_ages):\n","      # Use the first detected age for driving video selection\n","      driving_video_path = get_driving_video_path(ages[0])  # Use the first age in the list\n","      output_video = fomm_process(image_path, driving_video_path, output_videos_dir)\n","      output_videos.append(output_video)\n","\n","    # Output video paths\n","    for idx, video in enumerate(output_videos):\n","      print(f\"Output video for image {idx + 1}: {video}\")\n","\n","\n","\n","    # Background removal Module\n","\n","    # Initialize MediaPipe variables\n","    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n","\n","    # Paths\n","    input_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/output_videos\"\n","    bg_removed_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos\"\n","\n","    # Ensure output directory exists\n","    os.makedirs(bg_removed_videos_dir, exist_ok=True)\n","\n","    # Function to remove background and save video with black background\n","    def remove_background(input_video_path, output_video_path):\n","        cap = cv2.VideoCapture(input_video_path)\n","        if not cap.isOpened():\n","            print(f\"Error: Could not open video file {input_video_path}\")\n","            return\n","\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","        # Use MP4 codec\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height), isColor=True)\n","\n","        with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as selfie_segmentation:\n","            while cap.isOpened():\n","                ret, frame = cap.read()\n","                if not ret:\n","                    break\n","\n","                # Convert the frame to RGB\n","                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                results = selfie_segmentation.process(rgb_frame)\n","\n","                # Create the binary mask for segmentation\n","                condition = results.segmentation_mask > 0.5\n","\n","                # Apply the mask to set background pixels to black\n","                output_frame = np.zeros_like(frame)  # Start with a black background\n","                output_frame[condition] = frame[condition]  # Set person pixels from original frame\n","\n","                # Write the frame with the black background\n","                out.write(output_frame)\n","\n","        cap.release()\n","        out.release()\n","        print(f\"Background-removed video saved at {output_video_path}\")\n","\n","    # Process each video in the input directory\n","    for video_file in os.listdir(input_videos_dir):\n","        if video_file.endswith('.mp4'):\n","            input_video_path = os.path.join(input_videos_dir, video_file)\n","            output_video_path = os.path.join(bg_removed_videos_dir, video_file.replace('.mp4', '_no_bg.mp4'))\n","            remove_background(input_video_path, output_video_path)\n","\n","    # video concatenation Module\n","\n","    # Paths\n","    bg_removed_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos\"\n","    merged_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\"\n","\n","    # Get all background-removed video files\n","    bg_removed_videos = sorted([os.path.join(bg_removed_videos_dir, f) for f in os.listdir(bg_removed_videos_dir) if f.endswith('.mp4')])\n","\n","    # Load video clips\n","    clips = [VideoFileClip(video).resize((256, 256)) for video in bg_removed_videos]\n","\n","    # Concatenate clips horizontally\n","    final_clip = clips_array([[clips[0], clips[1], clips[2]]])  # Adjust this if you have more or fewer clips\n","\n","    # Write the output video\n","    final_clip.write_videofile(merged_video_path, codec='libx264', fps=30)\n","\n","    print(f\"Concatenated video saved at: {merged_video_path}\")\n","\n","    # Define paths\n","    merged_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\"\n","    merged_video_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_videoframes/\"\n","    background_video_tracker = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/background_video_tracker.txt\"\n","    background_video_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/\"\n","    background_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/framesbackgroundvideo/\"\n","    output_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/Untitled Folder/\"\n","    final_output_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/final_outputvideo/final_output_video.mp4\"\n","\n","\n","    # Utility: Clear frames from a folder without deleting the folder itself\n","    def clear_folder(folder_path):\n","        if os.path.exists(folder_path):\n","            for file in os.listdir(folder_path):\n","                file_path = os.path.join(folder_path, file)\n","                if os.path.isfile(file_path) or os.path.islink(file_path):\n","                    os.unlink(file_path)  # Remove files or links\n","                elif os.path.isdir(file_path):\n","                    shutil.rmtree(file_path)  # Remove subdirectories\n","        else:\n","            os.makedirs(folder_path)  # Create the folder if it doesn't exist\n","\n","\n","    # Dynamic background video selection\n","    def load_background_video():\n","        # Read current tracker value\n","        with open(background_video_tracker, \"r\") as file:\n","            current_index = int(file.read().strip())\n","\n","        # Update tracker to next video\n","        new_index = (current_index % 6) + 1\n","        with open(background_video_tracker, \"w\") as file:\n","            file.write(str(new_index))\n","\n","        # Select background video path\n","        background_video_name = f\"backgroundvideo{new_index}.mp4\"\n","        background_video_path = os.path.join(background_video_dir, background_video_name)\n","        print(f\"Selected Background Video: {background_video_path}\")\n","        return background_video_path\n","\n","\n","    # Clear folders before processing\n","    clear_folder(merged_video_frames_path)\n","    clear_folder(background_frames_path)\n","    clear_folder(output_frames_path)\n","\n","    # Step 1: Extract frames from a video\n","    def extract_frames_from_video(video_path, output_frames_path):\n","        cap = cv2.VideoCapture(video_path)\n","        frame_count = 0\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame_filename = os.path.join(output_frames_path, f'frame_{frame_count:04d}.png')\n","            cv2.imwrite(frame_filename, frame)\n","            frame_count += 1\n","        cap.release()\n","        print(f\"Extracted {frame_count} frames from {video_path}.\")\n","        return frame_count\n","\n","    # Step 2: Replace black background with transparency\n","    def replace_black_background_with_transparency(frames_path):\n","        frame_files = sorted(os.listdir(frames_path))\n","        for frame_file in frame_files:\n","            frame_path = os.path.join(frames_path, frame_file)\n","            frame = cv2.imread(frame_path)\n","            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","            lower_black = np.array([0, 0, 0])\n","            upper_black = np.array([180, 255, 30])\n","            mask = cv2.inRange(hsv, lower_black, upper_black)\n","            frame_bgra = cv2.cvtColor(frame, cv2.COLOR_BGR2BGRA)\n","            frame_bgra[mask != 0] = [0, 0, 0, 0]\n","            cv2.imwrite(frame_path, frame_bgra)\n","\n","    # Step 3: Overlay transparent frames onto background frames\n","    def overlay_frames(transparent_frames_path, background_frames_path, output_frames_path):\n","        transparent_frame_files = sorted(os.listdir(transparent_frames_path))\n","        background_frame_files = sorted(os.listdir(background_frames_path))\n","\n","        # Align frame counts by trimming extra frames from the longer video\n","        min_frame_count = min(len(transparent_frame_files), len(background_frame_files))\n","        transparent_frame_files = transparent_frame_files[:min_frame_count]\n","        background_frame_files = background_frame_files[:min_frame_count]\n","\n","        for i, frame_file in enumerate(transparent_frame_files):\n","            transparent_frame = cv2.imread(os.path.join(transparent_frames_path, frame_file), cv2.IMREAD_UNCHANGED)\n","            background_frame = cv2.imread(os.path.join(background_frames_path, background_frame_files[i]))\n","\n","            if transparent_frame.shape[2] == 4:  # Check for alpha channel\n","                alpha_channel = transparent_frame[:, :, 3] / 255.0\n","                for c in range(3):  # Blend RGB channels\n","                    background_frame[:, :, c] = background_frame[:, :, c] * (1 - alpha_channel) + transparent_frame[:, :, c] * alpha_channel\n","\n","            output_frame_path = os.path.join(output_frames_path, f'output_frame_{i:04d}.png')\n","            cv2.imwrite(output_frame_path, background_frame)\n","\n","        print(f\"Overlay complete. {min_frame_count} frames processed.\")\n","\n","    # Step 4: Combine frames into a video\n","    def create_video_from_frames(frames_path, output_video_path, fps=30):\n","        frame_files = sorted(os.listdir(frames_path))\n","        if not frame_files:\n","            raise ValueError(\"No frames found to create a video.\")\n","\n","        first_frame = cv2.imread(os.path.join(frames_path, frame_files[0]))\n","        height, width, _ = first_frame.shape\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n","\n","        for frame_file in frame_files:\n","            frame = cv2.imread(os.path.join(frames_path, frame_file))\n","            video_writer.write(frame)\n","\n","        video_writer.release()\n","        print(f\"Video saved at {output_video_path}.\")\n","\n","    # Execute the pipeline\n","    background_video_path = load_background_video()  # Load dynamic background video\n","    merged_frame_count = extract_frames_from_video(merged_video_path, merged_video_frames_path)\n","    replace_black_background_with_transparency(merged_video_frames_path)\n","    background_frame_count = extract_frames_from_video(background_video_path, background_frames_path)\n","    overlay_frames(merged_video_frames_path, background_frames_path, output_frames_path)\n","    create_video_from_frames(output_frames_path, final_output_video_path, fps=30)\n","\n","    return age_message, \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/final_outputvideo/final_output_video.mp4\"\n","\n","\n","# UI Layout\n","# UI Layout\n","def create_ui():\n","    # Define UI elements\n","    with gr.Blocks(css=\".gradio-container {background-color: black; color: white;}\") as demo:\n","        # Title\n","        gr.Markdown(\"<h1 style='text-align: center; color: white;'>Generate Age-specific Animation using AI</h1>\")\n","\n","        # Display Image (Introductory image preview)\n","        gr.Image(\n","            value=\"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/websiteimagesample.png\",\n","            label=\"Animation Preview\",\n","            interactive=False\n","        )\n","\n","        # Upload Section\n","        gr.Markdown(\"<h2 style='text-align: center; color: white;'>Upload 3 Images!</h2>\")\n","\n","        # Image Upload Boxes with Preview\n","        with gr.Row():\n","            image1 = gr.Image(label=\"Image 1\", interactive=True, type=\"filepath\")\n","            image2 = gr.Image(label=\"Image 2\", interactive=True, type=\"filepath\")\n","            image3 = gr.Image(label=\"Image 3\", interactive=True, type=\"filepath\")\n","\n","        # Generate Video Button\n","        generate_button = gr.Button(\"Generate Video\")\n","\n","        # Detected Ages Textbox\n","        output_ages = gr.Textbox(label=\"Detected Ages\", interactive=False)\n","\n","        # Playable Video Section\n","        output_video = gr.Video(label=\"Generated Video\")\n","\n","\n","\n","        # Button Click Action\n","        generate_button.click(\n","            fn=agespecificmotion,  # Call the external function\n","            inputs=[image1, image2, image3],  # Pass inputs to the function\n","            outputs=[output_ages, output_video]  # Display both ages and video\n","        )\n","\n","    return demo\n","\n","# Launch the UI\n","create_ui().launch()\n"],"metadata":{"id":"0UhuFdL8OtXj","colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"status":"ok","timestamp":1734764762793,"user_tz":-330,"elapsed":2896,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"}},"outputId":"4dff32a0-7843-4129-8ccb-2e8b8e2c4537"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://79563e6cdd7c72f53a.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://79563e6cdd7c72f53a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["#final with enhancement\n","import cv2 as cv\n","import sys\n","sys.path.append('/content/drive/MyDrive/project_files/age_and_gender_detection')\n","sys.path.append('/content/drive/MyDrive/project_files/age_and_gender_detection/modules')\n","import PIL.Image\n","import cv2\n","import ffmpeg\n","import imageio\n","import numpy as np\n","import os\n","import warnings\n","from base64 import b64encode\n","from demo import load_checkpoints, make_animation  # Ensure these functions are correctly implemented\n","from skimage import img_as_ubyte\n","from tempfile import NamedTemporaryFile\n","from tqdm.auto import tqdm\n","import cv2\n","import mediapipe as mp\n","import numpy as np\n","import os\n","import cv2\n","import os\n","import numpy as np\n","import shutil\n","from moviepy.editor import VideoFileClip, clips_array\n","import os\n","import gradio as gr\n","from moviepy.editor import VideoFileClip\n","\n","def agespecificmotion(img1, img2, img3):\n","    \"\"\"\n","    A single function to handle the entire pipeline for generating an age-specific animation.\n","    Takes three uploaded image paths as input.\n","    \"\"\"\n","    # Save uploaded images to specific paths\n","    saved_paths = [\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image1.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image2.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image3.png\"\n","    ]\n","    # Save images to predefined paths\n","    for img, path in zip([img1, img2, img3], saved_paths):\n","        import shutil\n","        shutil.copy(img, path)\n","\n","    # Place all the pipeline code (modules/functions) here\n","\n","\n","\n","    # Age and Gender Detection Module\n","    def getFaceBox(net, frame, conf_threshold=0.7):\n","        frameHeight = frame.shape[0]\n","        frameWidth = frame.shape[1]\n","        blob = cv.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], True, False)\n","\n","        net.setInput(blob)\n","        detections = net.forward()\n","        bboxes = []\n","        for i in range(detections.shape[2]):\n","            confidence = detections[0, 0, i, 2]\n","            if confidence > conf_threshold:\n","                x1 = int(detections[0, 0, i, 3] * frameWidth)\n","                y1 = int(detections[0, 0, i, 4] * frameHeight)\n","                x2 = int(detections[0, 0, i, 5] * frameWidth)\n","                y2 = int(detections[0, 0, i, 6] * frameHeight)\n","                bboxes.append([x1, y1, x2, y2])\n","        return bboxes\n","\n","    faceProto = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/opencv_face_detector.pbtxt\"\n","    faceModel = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/opencv_face_detector_uint8.pb\"\n","\n","    ageProto = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/age_deploy.prototxt'\n","    ageModel = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/age_net.caffemodel'\n","    genderProto = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/gender_deploy.prototxt'\n","    genderModel = '/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/modelNweight/gender_net.caffemodel'\n","\n","    MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n","    ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-55)', '(56-70)', '(71-100)']\n","    genderList = ['Male', 'Female']\n","\n","    # Load network\n","    ageNet = cv.dnn.readNet(ageModel, ageProto)\n","    genderNet = cv.dnn.readNet(genderModel, genderProto)\n","    faceNet = cv.dnn.readNet(faceModel, faceProto)\n","\n","    padding = 20\n","\n","    def age_gender_detector(frame):\n","        bboxes = getFaceBox(faceNet, frame)\n","        ages = []\n","        for bbox in bboxes:\n","            face = frame[max(0, bbox[1]-padding):min(bbox[3]+padding, frame.shape[0]-1),\n","                        max(0, bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n","\n","            blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n","            genderNet.setInput(blob)\n","            genderPreds = genderNet.forward()\n","            ageNet.setInput(blob)\n","            agePreds = ageNet.forward()\n","\n","            # Get the detected age range\n","            age_range = ageList[agePreds[0].argmax()]  # Get the age category\n","\n","            # Convert age range to midpoint integer (used for the next module)\n","            age_min, age_max = map(int, age_range.strip('()').split('-'))\n","            age_mid = (age_min + age_max) // 2\n","            ages.append(age_mid)  # Store the midpoint integer\n","\n","        return ages  # Return midpoints for the next module\n","\n","    # List of input image paths (change the paths to your images)\n","    image_paths = [\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image1.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image2.png\",\n","        \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image3.png\"\n","    ]\n","\n","    # Variables to store detected ages for each image\n","    detected_ages = []  # To hold ages for all images\n","    all_age_categories = []\n","    for idx, image_path in enumerate(image_paths):\n","        input_image = cv.imread(image_path)\n","        ages = age_gender_detector(input_image)  # Get detected ages (midpoints) for the current image\n","        detected_ages.append(ages)  # Append to the detected_ages list\n","\n","    # Output the detected age categories for the print statement (just categories, not exact ages)\n","    for idx, age in enumerate(detected_ages):\n","        # Get the age categories for printing based on midpoint\n","        age_categories = []\n","        for mid_age in age:\n","            # Match the midpoint to its category\n","            for category in ageList:\n","                age_min, age_max = map(int, category.strip('()').split('-'))\n","                if age_min <= mid_age <= age_max:\n","                    age_categories.append(category)\n","                    break\n","        all_age_categories.append(age_categories)\n","        print(f\"Detected Age Categories for Image {idx + 1}: {', '.join(age_categories)}\")\n","    age_message = (f\"The person in the first picture belongs to the age category {all_age_categories[0][0]}, \"\n","               f\"the person in the second picture belongs to the age category {all_age_categories[1][0]}, \"\n","               f\"and the person in the third picture belongs to the age category {all_age_categories[2][0]}.\")\n","\n","    # First order motion model Module\n","\n","    warnings.filterwarnings(\"ignore\")\n","\n","    # Paths for images and age motion videos\n","    image_paths = [\n","      \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image1.png\",\n","      \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image2.png\",\n","      \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/inputimages/image3.png\"\n","    ]\n","\n","    age_motion_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/agemotionvideos\"\n","    output_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/output_videos\"\n","\n","    # Ensure output directory exists\n","    os.makedirs(output_videos_dir, exist_ok=True)\n","\n","    # Function to get the driving video path based on the age, alternating videos if multiple inputs fall in the same category\n","    category_counter = {\n","      '7-12': 0,\n","      '21-37': 0,\n","      '13-20':0\n","    }\n","\n","    def get_driving_video_path(age):\n","      if age < 2:\n","          return os.path.join(age_motion_videos_dir, '0to2.mp4')\n","      elif 3 <= age < 6:\n","          return os.path.join(age_motion_videos_dir, '4to6.mp4')\n","      elif 7 <= age < 12:\n","          video_file = f'8to12v{category_counter[\"7-12\"] % 2 + 1}.mp4'\n","          category_counter['7-12'] += 1\n","          return os.path.join(age_motion_videos_dir, video_file)\n","      elif 13 <= age < 20:\n","          video_file = f'13to20v{category_counter[\"13-20\"] % 2 + 1}.mp4'\n","          category_counter['13-20'] += 1\n","          return os.path.join(age_motion_videos_dir, video_file)\n","\n","      elif 21 <= age < 37:\n","          video_file = f'21to37v{category_counter[\"21-37\"] % 2 + 1}.mp4'\n","          category_counter['21-37'] += 1\n","          return os.path.join(age_motion_videos_dir, video_file)\n","      elif 38 <= age < 55:\n","          return os.path.join(age_motion_videos_dir, '38to55.mp4')\n","      elif 56 <= age < 70:\n","          return os.path.join(age_motion_videos_dir, '56to70.mp4')\n","      elif 71 <= age < 100:\n","          return os.path.join(age_motion_videos_dir, '71to100.mp4')\n","\n","    # Function to resize images to (256, 256)\n","    def resize_image(image):\n","      return image.resize((256, 256), resample=PIL.Image.LANCZOS)\n","\n","    # FOMM processing function\n","    def fomm_process(image_path, driving_video_path, output_dir):\n","      # Load and resize the input image\n","      input_image = PIL.Image.open(image_path).convert(\"RGB\")\n","      input_image_resized = resize_image(input_image)\n","\n","      # Load the driving video\n","      reader = imageio.get_reader(driving_video_path, mode='I', format='FFMPEG')\n","      fps = reader.get_meta_data()['fps']\n","      driving_video = [frame for frame in reader]\n","\n","      # Load the FOMM model\n","      model_name = 'vox'  # Model name\n","      checkpoint_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/config/vox-256.yaml\"\n","      weights_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/checkpoints/vox-cpk.pth\"\n","\n","      generator, kp_detector = load_checkpoints(config_path=checkpoint_path, checkpoint_path=weights_path)\n","\n","      # Generate the animation\n","      predictions = make_animation(\n","          np.array(input_image_resized) / 255.0,  # Normalize the image\n","          [cv2.resize(frame, (256, 256)) / 255.0 for frame in driving_video],  # Normalize frames\n","          generator,\n","          kp_detector,\n","          relative=True,  # Adjust as necessary\n","          adapt_movement_scale=True  # Adjust as necessary\n","      )\n","\n","      # Save the output video\n","      output_video_path = os.path.join(output_dir, os.path.basename(image_path).split('.')[0] + '_output.mp4')\n","      imageio.mimsave(output_video_path, [img_as_ubyte(frame) for frame in predictions], fps=fps)\n","\n","      return output_video_path\n","\n","    # Example usage\n","    detected_ages = []  # This will hold the ages from the age detection module\n","\n","    # Assuming this part of the code is executed after running the age detection module\n","    for idx, image_path in enumerate(image_paths):\n","      input_image = cv.imread(image_path)\n","      ages = age_gender_detector(input_image)  # Get detected ages for the current image\n","      detected_ages.append(ages)  # Append to the detected_ages list\n","\n","    output_videos = []\n","    for image_path, ages in zip(image_paths, detected_ages):\n","      # Use the first detected age for driving video selection\n","      driving_video_path = get_driving_video_path(ages[0])  # Use the first age in the list\n","      output_video = fomm_process(image_path, driving_video_path, output_videos_dir)\n","      output_videos.append(output_video)\n","\n","    # Output video paths\n","    for idx, video in enumerate(output_videos):\n","      print(f\"Output video for image {idx + 1}: {video}\")\n","\n","\n","\n","    # Background removal Module\n","\n","    # Initialize MediaPipe variables\n","    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n","\n","    # Paths\n","    input_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/output_videos\"\n","    bg_removed_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos\"\n","\n","    # Ensure output directory exists\n","    os.makedirs(bg_removed_videos_dir, exist_ok=True)\n","\n","    # Function to remove background and save video with black background\n","    def remove_background(input_video_path, output_video_path):\n","        cap = cv2.VideoCapture(input_video_path)\n","        if not cap.isOpened():\n","            print(f\"Error: Could not open video file {input_video_path}\")\n","            return\n","\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","        # Use MP4 codec\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height), isColor=True)\n","\n","        with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as selfie_segmentation:\n","            while cap.isOpened():\n","                ret, frame = cap.read()\n","                if not ret:\n","                    break\n","\n","                # Convert the frame to RGB\n","                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                results = selfie_segmentation.process(rgb_frame)\n","\n","                # Create the binary mask for segmentation\n","                condition = results.segmentation_mask > 0.5\n","\n","                # Apply the mask to set background pixels to black\n","                output_frame = np.zeros_like(frame)  # Start with a black background\n","                output_frame[condition] = frame[condition]  # Set person pixels from original frame\n","\n","                # Write the frame with the black background\n","                out.write(output_frame)\n","\n","        cap.release()\n","        out.release()\n","        print(f\"Background-removed video saved at {output_video_path}\")\n","\n","    # Process each video in the input directory\n","    for video_file in os.listdir(input_videos_dir):\n","        if video_file.endswith('.mp4'):\n","            input_video_path = os.path.join(input_videos_dir, video_file)\n","            output_video_path = os.path.join(bg_removed_videos_dir, video_file.replace('.mp4', '_no_bg.mp4'))\n","            remove_background(input_video_path, output_video_path)\n","\n","\n","\n","    bg_removed_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos\"\n","    bg_removed_videos = sorted([os.path.join(bg_removed_videos_dir, f) for f in os.listdir(bg_removed_videos_dir) if f.endswith('_no_bg.mp4')])\n","\n","    def trim_videos(video_paths):\n","        # Load video clips and get their durations\n","        clips = []\n","        durations = []\n","\n","        for path in video_paths:\n","            try:\n","                clip = VideoFileClip(path)\n","                clips.append(clip)\n","                durations.append(clip.duration)\n","            except Exception as e:\n","                print(f\"Error loading video {path}: {e}\")\n","\n","        # Ensure we have valid clips to work with\n","        if not clips:\n","            print(\"No valid clips loaded for trimming.\")\n","            return\n","\n","        # Find the minimum duration in seconds\n","        min_duration = min(durations)\n","        print(f\"Minimum duration found: {min_duration:.3f} seconds\")\n","\n","        # Trim each video to the minimum duration\n","        for i, clip in enumerate(clips):\n","            try:\n","                current_duration = clip.duration\n","\n","                # Check if the current video needs trimming\n","                if current_duration > min_duration:\n","                    # Trim video to the minimum duration\n","                    trimmed_clip = clip.subclip(0, min_duration)\n","\n","                    # Write the trimmed video back to the same file\n","                    trimmed_clip.write_videofile(video_paths[i], codec='libx264', audio_codec='aac', remove_temp=True)\n","                    trimmed_clip.close()  # Close the trimmed clip\n","                    print(f\"Trimmed {video_paths[i]} to {min_duration:.3f} seconds\")\n","                else:\n","                    print(f\"{video_paths[i]} is already at or below the minimum duration.\")\n","\n","            except Exception as e:\n","                print(f\"Error trimming video {video_paths[i]}: {e}\")\n","\n","        # Close all original clips\n","        for clip in clips:\n","            clip.close()\n","\n","\n","\n","    # video concatenation Module\n","\n","    # Paths\n","    bg_removed_videos_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/bg_removed_videos\"\n","    merged_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\"\n","\n","\n","    # Get all background-removed video files\n","    bg_removed_videos = sorted([os.path.join(bg_removed_videos_dir, f) for f in os.listdir(bg_removed_videos_dir) if f.endswith('.mp4')])\n","\n","    # Load video clips\n","    clips = [VideoFileClip(video).resize((256, 256)) for video in bg_removed_videos]\n","\n","    # Concatenate clips horizontally\n","    final_clip = clips_array([[clips[0], clips[1], clips[2]]])  # Adjust this if you have more or fewer clips\n","\n","    # Write the output video\n","    final_clip.write_videofile(merged_video_path, codec='libx264', fps=30)\n","\n","    print(f\"Concatenated video saved at: {merged_video_path}\")\n","\n","    # Define paths\n","    merged_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_video.mp4\"\n","    merged_video_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/merged_video/merged_videoframes/\"\n","    background_video_tracker = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/background_video_tracker.txt\"\n","    background_video_dir = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/\"\n","    background_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/framesbackgroundvideo/\"\n","    output_frames_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/Untitled Folder/\"\n","    final_output_video_path = \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/final_outputvideo/final_output_video.mp4\"\n","\n","\n","    # Utility: Clear frames from a folder without deleting the folder itself\n","    def clear_folder(folder_path):\n","        if os.path.exists(folder_path):\n","            for file in os.listdir(folder_path):\n","                file_path = os.path.join(folder_path, file)\n","                if os.path.isfile(file_path) or os.path.islink(file_path):\n","                    os.unlink(file_path)  # Remove files or links\n","                elif os.path.isdir(file_path):\n","                    shutil.rmtree(file_path)  # Remove subdirectories\n","        else:\n","            os.makedirs(folder_path)  # Create the folder if it doesn't exist\n","\n","\n","    # Dynamic background video selection\n","    def load_background_video():\n","        # Read current tracker value\n","        with open(background_video_tracker, \"r\") as file:\n","            current_index = int(file.read().strip())\n","\n","        # Update tracker to next video\n","        new_index = (current_index % 6) + 1\n","        with open(background_video_tracker, \"w\") as file:\n","            file.write(str(new_index))\n","\n","        # Select background video path\n","        background_video_name = f\"backgroundvideo{new_index}.mp4\"\n","        background_video_path = os.path.join(background_video_dir, background_video_name)\n","        print(f\"Selected Background Video: {background_video_path}\")\n","        return background_video_path\n","\n","\n","    # Clear folders before processing\n","    clear_folder(merged_video_frames_path)\n","    clear_folder(background_frames_path)\n","    clear_folder(output_frames_path)\n","\n","    # Step 1: Extract frames from a video\n","    def extract_frames_from_video(video_path, output_frames_path):\n","        cap = cv2.VideoCapture(video_path)\n","        frame_count = 0\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame_filename = os.path.join(output_frames_path, f'frame_{frame_count:04d}.png')\n","            cv2.imwrite(frame_filename, frame)\n","            frame_count += 1\n","        cap.release()\n","        print(f\"Extracted {frame_count} frames from {video_path}.\")\n","        return frame_count\n","\n","    # Step 2: Replace black background with transparency\n","    def replace_black_background_with_transparency(frames_path):\n","        frame_files = sorted(os.listdir(frames_path))\n","        for frame_file in frame_files:\n","            frame_path = os.path.join(frames_path, frame_file)\n","            frame = cv2.imread(frame_path)\n","            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","            lower_black = np.array([0, 0, 0])\n","            upper_black = np.array([180, 255, 30])\n","            mask = cv2.inRange(hsv, lower_black, upper_black)\n","            frame_bgra = cv2.cvtColor(frame, cv2.COLOR_BGR2BGRA)\n","            frame_bgra[mask != 0] = [0, 0, 0, 0]\n","            cv2.imwrite(frame_path, frame_bgra)\n","\n","    # Step 3: Overlay transparent frames onto background frames\n","    def overlay_frames(transparent_frames_path, background_frames_path, output_frames_path):\n","        transparent_frame_files = sorted(os.listdir(transparent_frames_path))\n","        background_frame_files = sorted(os.listdir(background_frames_path))\n","\n","        # Align frame counts by trimming extra frames from the longer video\n","        min_frame_count = min(len(transparent_frame_files), len(background_frame_files))\n","        transparent_frame_files = transparent_frame_files[:min_frame_count]\n","        background_frame_files = background_frame_files[:min_frame_count]\n","\n","        for i, frame_file in enumerate(transparent_frame_files):\n","            transparent_frame = cv2.imread(os.path.join(transparent_frames_path, frame_file), cv2.IMREAD_UNCHANGED)\n","            background_frame = cv2.imread(os.path.join(background_frames_path, background_frame_files[i]))\n","\n","            if transparent_frame.shape[2] == 4:  # Check for alpha channel\n","                alpha_channel = transparent_frame[:, :, 3] / 255.0\n","                for c in range(3):  # Blend RGB channels\n","                    background_frame[:, :, c] = background_frame[:, :, c] * (1 - alpha_channel) + transparent_frame[:, :, c] * alpha_channel\n","\n","            output_frame_path = os.path.join(output_frames_path, f'output_frame_{i:04d}.png')\n","            cv2.imwrite(output_frame_path, background_frame)\n","\n","        print(f\"Overlay complete. {min_frame_count} frames processed.\")\n","\n","    # Step 4: Combine frames into a video\n","    def create_video_from_frames(frames_path, output_video_path, fps=30):\n","        frame_files = sorted(os.listdir(frames_path))\n","        if not frame_files:\n","            raise ValueError(\"No frames found to create a video.\")\n","\n","        first_frame = cv2.imread(os.path.join(frames_path, frame_files[0]))\n","        height, width, _ = first_frame.shape\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n","\n","        for frame_file in frame_files:\n","            frame = cv2.imread(os.path.join(frames_path, frame_file))\n","            video_writer.write(frame)\n","\n","        video_writer.release()\n","        print(f\"Video saved at {output_video_path}.\")\n","\n","    # Execute the pipeline\n","    background_video_path = load_background_video()  # Load dynamic background video\n","    merged_frame_count = extract_frames_from_video(merged_video_path, merged_video_frames_path)\n","    replace_black_background_with_transparency(merged_video_frames_path)\n","    background_frame_count = extract_frames_from_video(background_video_path, background_frames_path)\n","    overlay_frames(merged_video_frames_path, background_frames_path, output_frames_path)\n","    create_video_from_frames(output_frames_path, final_output_video_path, fps=30)\n","\n","    return age_message, \"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/final_outputvideo/final_output_video.mp4\"\n","\n","\n","# UI Layout\n","# UI Layout\n","def create_ui():\n","    # Define UI elements\n","    with gr.Blocks(css=\".gradio-container {background-color: black; color: white;}\") as demo:\n","        # Title\n","        gr.Markdown(\"<h1 style='text-align: center; color: white;'>Generate Age-specific Animation using AI</h1>\")\n","\n","        # Display Image (Introductory image preview)\n","        gr.Image(\n","            value=\"/content/drive/MyDrive/Generating_age_specific_motion_from_images_using_deep_learning/project_files/age_and_gender_detection/websiteimagesample.png\",\n","            label=\"Animation Preview\",\n","            interactive=False\n","        )\n","\n","        # Upload Section\n","        gr.Markdown(\"<h2 style='text-align: center; color: white;'>Upload 3 Images!</h2>\")\n","\n","        # Image Upload Boxes with Preview\n","        with gr.Row():\n","            image1 = gr.Image(label=\"Image 1\", interactive=True, type=\"filepath\")\n","            image2 = gr.Image(label=\"Image 2\", interactive=True, type=\"filepath\")\n","            image3 = gr.Image(label=\"Image 3\", interactive=True, type=\"filepath\")\n","\n","        # Generate Video Button\n","        generate_button = gr.Button(\"Generate Video\")\n","\n","        # Detected Ages Textbox\n","        output_ages = gr.Textbox(label=\"Detected Ages\", interactive=False)\n","\n","        # Playable Video Section\n","        output_video = gr.Video(label=\"Generated Video\")\n","\n","\n","\n","        # Button Click Action\n","        generate_button.click(\n","            fn=agespecificmotion,  # Call the external function\n","            inputs=[image1, image2, image3],  # Pass inputs to the function\n","            outputs=[output_ages, output_video]  # Display both ages and video\n","        )\n","\n","    return demo\n","\n","# Launch the UI\n","create_ui().launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"hnaHISJNtaSg","executionInfo":{"status":"ok","timestamp":1734765761398,"user_tz":-330,"elapsed":3645,"user":{"displayName":"Amritha priyadarshni","userId":"10424081276664681294"}},"outputId":"b02e2f05-0d17-4bdc-81ca-27d599e13c6a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://d2cff70cb50281fa33.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://d2cff70cb50281fa33.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"InmBT5A36GRp"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}