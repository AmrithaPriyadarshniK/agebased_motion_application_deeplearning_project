{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcrN8vZVmPq9"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "zdeqgq3omfTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "from google.colab import files\n",
        "\n",
        "!pip install opencv-python-headless tensorflow\n",
        "!pip install mediapipe opencv-python\n",
        "!pip install gradio\n",
        "!pip install rembg onnxruntime\n",
        "# First, install the rembg library if you haven't already\n",
        "!pip install rembg\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install moviepy\n",
        "!pip install -r /content/drive/MyDrive/agebased_motion_application_deeplearning_project/requirements.txt\n",
        "!pip install sync-batchnorm\n",
        "!pip install opencv-python-headless tensorflow\n",
        "!pip install ffmpeg-python\n",
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "qsKKM2FHmhQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Integrated module\n",
        "import cv2 as cv\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/agebased_motion_application_deeplearning_project/modules') # Add this line with the correct path to demo.py\n",
        "from demo import load_checkpoints, make_animation\n",
        "sys.path.append('/content/drive/MyDrive/agebased_motion_application_deeplearning_project')\n",
        "sys.path.append('/content/drive/MyDrive/agebased_motion_application_deeplearning_project/modules')\n",
        "import PIL.Image\n",
        "import ffmpeg\n",
        "import imageio\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "from base64 import b64encode\n",
        "from skimage import img_as_ubyte\n",
        "from tempfile import NamedTemporaryFile\n",
        "from tqdm.auto import tqdm\n",
        "import mediapipe as mp\n",
        "import shutil\n",
        "from moviepy.editor import VideoFileClip, clips_array\n",
        "import gradio as gr\n",
        "\n",
        "def agespecificmotion(img1, img2, img3):\n",
        "    \"\"\"\n",
        "    A single function to handle the entire pipeline for generating an age-specific animation.\n",
        "    Takes three uploaded image paths as input.\n",
        "    \"\"\"\n",
        "    # Save uploaded images to specific paths\n",
        "    saved_paths = [\n",
        "        \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/inputimages/image1.png\",\n",
        "        \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/inputimages/image2.png\",\n",
        "        \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/inputimages/image3.png\"\n",
        "    ]\n",
        "    # Save images to predefined paths\n",
        "    for img, path in zip([img1, img2, img3], saved_paths):\n",
        "        import shutil\n",
        "        shutil.copy(img, path)\n",
        "\n",
        "    # pipeline code (modules/functions)\n",
        "    # Age and Gender Detection Module\n",
        "    def getFaceBox(net, frame, conf_threshold=0.7):\n",
        "        frameHeight = frame.shape[0]\n",
        "        frameWidth = frame.shape[1]\n",
        "        blob = cv.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], True, False)\n",
        "\n",
        "        net.setInput(blob)\n",
        "        detections = net.forward()\n",
        "        bboxes = []\n",
        "        for i in range(detections.shape[2]):\n",
        "            confidence = detections[0, 0, i, 2]\n",
        "            if confidence > conf_threshold:\n",
        "                x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
        "                y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
        "                x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
        "                y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
        "                bboxes.append([x1, y1, x2, y2])\n",
        "        return bboxes\n",
        "\n",
        "    faceProto = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/modelNweight/opencv_face_detector.pbtxt\"\n",
        "    faceModel = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/modelNweight/opencv_face_detector_uint8.pb\"\n",
        "    ageProto = '/content/drive/MyDrive/agebased_motion_application_deeplearning_project/modelNweight/age_deploy.prototxt'\n",
        "    ageModel = '/content/drive/MyDrive/agebased_motion_application_deeplearning_project/modelNweight/age_net.caffemodel'\n",
        "    genderProto = '/content/drive/MyDrive/agebased_motion_application_deeplearning_project/modelNweight/gender_deploy.prototxt'\n",
        "    genderModel = '/content/drive/MyDrive/agebased_motion_application_deeplearning_project/modelNweight/gender_net.caffemodel'\n",
        "\n",
        "    MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
        "    ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-55)', '(56-70)', '(71-100)']\n",
        "    genderList = ['Male', 'Female']\n",
        "\n",
        "    # Load network\n",
        "    ageNet = cv.dnn.readNet(ageModel, ageProto)\n",
        "    genderNet = cv.dnn.readNet(genderModel, genderProto)\n",
        "    faceNet = cv.dnn.readNet(faceModel, faceProto)\n",
        "\n",
        "    padding = 20\n",
        "\n",
        "    def age_gender_detector(frame):\n",
        "        bboxes = getFaceBox(faceNet, frame)\n",
        "        ages = []\n",
        "        for bbox in bboxes:\n",
        "            face = frame[max(0, bbox[1]-padding):min(bbox[3]+padding, frame.shape[0]-1),\n",
        "                        max(0, bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
        "\n",
        "            blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
        "            genderNet.setInput(blob)\n",
        "            genderPreds = genderNet.forward()\n",
        "            ageNet.setInput(blob)\n",
        "            agePreds = ageNet.forward()\n",
        "\n",
        "            # Get the detected age range\n",
        "            age_range = ageList[agePreds[0].argmax()]  # Get the age category\n",
        "\n",
        "            # Convert age range to midpoint integer (used for the next module)\n",
        "            age_min, age_max = map(int, age_range.strip('()').split('-'))\n",
        "            age_mid = (age_min + age_max) // 2\n",
        "            ages.append(age_mid)  # Store the midpoint integer\n",
        "\n",
        "        return ages  # Return midpoints for the next module\n",
        "\n",
        "    # List of input image paths (change the paths to your images)\n",
        "    image_paths = [\n",
        "        \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/inputimages/image1.png\",\n",
        "        \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/inputimages/image2.png\",\n",
        "        \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/inputimages/image3.png\"\n",
        "    ]\n",
        "\n",
        "    # Variables to store detected ages for each image\n",
        "    detected_ages = []  # To hold ages for all images\n",
        "    all_age_categories = []\n",
        "    for idx, image_path in enumerate(image_paths):\n",
        "        input_image = cv.imread(image_path)\n",
        "        ages = age_gender_detector(input_image)  # Get detected ages (midpoints) for the current image\n",
        "        detected_ages.append(ages)  # Append to the detected_ages list\n",
        "\n",
        "    # Output the detected age categories for the print statement (just categories, not exact ages)\n",
        "    for idx, age in enumerate(detected_ages):\n",
        "        # Get the age categories for printing based on midpoint\n",
        "        age_categories = []\n",
        "        for mid_age in age:\n",
        "            # Match the midpoint to its category\n",
        "            for category in ageList:\n",
        "                age_min, age_max = map(int, category.strip('()').split('-'))\n",
        "                if age_min <= mid_age <= age_max:\n",
        "                    age_categories.append(category)\n",
        "                    break\n",
        "        all_age_categories.append(age_categories)\n",
        "        print(f\"Detected Age Categories for Image {idx + 1}: {', '.join(age_categories)}\")\n",
        "    age_message = (f\"The person in the first picture belongs to the age category {all_age_categories[0][0]}, \"\n",
        "               f\"the person in the second picture belongs to the age category {all_age_categories[1][0]}, \"\n",
        "               f\"and the person in the third picture belongs to the age category {all_age_categories[2][0]}.\")\n",
        "\n",
        "    # First order motion model Module\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    age_motion_videos_dir = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/agemotionvideos\"\n",
        "    output_videos_dir = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/output_videos\"\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_videos_dir, exist_ok=True)\n",
        "\n",
        "    # Function to get the driving video path based on the age, alternating videos if multiple inputs fall in the same category\n",
        "    category_counter = {\n",
        "      '7-12': 0,\n",
        "      '21-37': 0,\n",
        "      '13-20':0\n",
        "    }\n",
        "\n",
        "    def get_driving_video_path(age):\n",
        "      if age < 2:\n",
        "          return os.path.join(age_motion_videos_dir, '0to2.mp4')\n",
        "      elif 3 <= age < 6:\n",
        "          return os.path.join(age_motion_videos_dir, '4to6.mp4')\n",
        "      elif 7 <= age < 12:\n",
        "          video_file = f'8to12v{category_counter[\"7-12\"] % 2 + 1}.mp4'\n",
        "          category_counter['7-12'] += 1\n",
        "          return os.path.join(age_motion_videos_dir, video_file)\n",
        "      elif 13 <= age < 20:\n",
        "          video_file = f'13to20v{category_counter[\"13-20\"] % 2 + 1}.mp4'\n",
        "          category_counter['13-20'] += 1\n",
        "          return os.path.join(age_motion_videos_dir, video_file)\n",
        "\n",
        "      elif 21 <= age < 37:\n",
        "          video_file = f'21to37v{category_counter[\"21-37\"] % 2 + 1}.mp4'\n",
        "          category_counter['21-37'] += 1\n",
        "          return os.path.join(age_motion_videos_dir, video_file)\n",
        "      elif 38 <= age < 55:\n",
        "          return os.path.join(age_motion_videos_dir, '38to55.mp4')\n",
        "      elif 56 <= age < 70:\n",
        "          return os.path.join(age_motion_videos_dir, '56to70.mp4')\n",
        "      elif 71 <= age < 100:\n",
        "          return os.path.join(age_motion_videos_dir, '71to100.mp4')\n",
        "\n",
        "    # Function to resize images to (256, 256)\n",
        "    def resize_image(image):\n",
        "      return image.resize((256, 256), resample=PIL.Image.LANCZOS)\n",
        "\n",
        "    # FOMM processing function\n",
        "    def fomm_process(image_path, driving_video_path, output_dir):\n",
        "      # Load and resize the input image\n",
        "      input_image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "      input_image_resized = resize_image(input_image)\n",
        "\n",
        "      # Load the driving video\n",
        "      reader = imageio.get_reader(driving_video_path, mode='I', format='FFMPEG')\n",
        "      fps = reader.get_meta_data()['fps']\n",
        "      driving_video = [frame for frame in reader]\n",
        "\n",
        "      # Load the FOMM model\n",
        "      model_name = 'vox'  # Model name\n",
        "      checkpoint_path = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/config/vox-256.yaml\"\n",
        "      weights_path = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/checkpoints/vox-cpk.pth\"\n",
        "\n",
        "      generator, kp_detector = load_checkpoints(config_path=checkpoint_path, checkpoint_path=weights_path)\n",
        "\n",
        "      # Generate the animation\n",
        "      predictions = make_animation(\n",
        "          np.array(input_image_resized) / 255.0,  # Normalize the image\n",
        "          [cv2.resize(frame, (256, 256)) / 255.0 for frame in driving_video],  # Normalize frames\n",
        "          generator,\n",
        "          kp_detector,\n",
        "          relative=True,  # Adjust as necessary\n",
        "          adapt_movement_scale=True  # Adjust as necessary\n",
        "      )\n",
        "\n",
        "      # Save the output video\n",
        "      output_video_path = os.path.join(output_dir, os.path.basename(image_path).split('.')[0] + '_output.mp4')\n",
        "      imageio.mimsave(output_video_path, [img_as_ubyte(frame) for frame in predictions], fps=fps)\n",
        "\n",
        "      return output_video_path\n",
        "\n",
        "    # Example usage\n",
        "    detected_ages = []  # This will hold the ages from the age detection module\n",
        "\n",
        "    # Assuming this part of the code is executed after running the age detection module\n",
        "    for idx, image_path in enumerate(image_paths):\n",
        "      input_image = cv.imread(image_path)\n",
        "      ages = age_gender_detector(input_image)  # Get detected ages for the current image\n",
        "      detected_ages.append(ages)  # Append to the detected_ages list\n",
        "\n",
        "    output_videos = []\n",
        "    for image_path, ages in zip(image_paths, detected_ages):\n",
        "      # Use the first detected age for driving video selection\n",
        "      driving_video_path = get_driving_video_path(ages[0])  # Use the first age in the list\n",
        "      output_video = fomm_process(image_path, driving_video_path, output_videos_dir)\n",
        "      output_videos.append(output_video)\n",
        "\n",
        "    # Output video paths\n",
        "    for idx, video in enumerate(output_videos):\n",
        "      print(f\"Output video for image {idx + 1}: {video}\")\n",
        "\n",
        "\n",
        "\n",
        "    # Background removal Module\n",
        "\n",
        "    # Initialize MediaPipe variables\n",
        "    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
        "\n",
        "    # Paths\n",
        "    input_videos_dir = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/output_videos\"\n",
        "    bg_removed_videos_dir = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/bg_removed_videos\"\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(bg_removed_videos_dir, exist_ok=True)\n",
        "\n",
        "    # Function to remove background and save video with black background\n",
        "    def remove_background(input_video_path, output_video_path):\n",
        "        cap = cv2.VideoCapture(input_video_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open video file {input_video_path}\")\n",
        "            return\n",
        "\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        # Use MP4 codec\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height), isColor=True)\n",
        "\n",
        "        with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as selfie_segmentation:\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Convert the frame to RGB\n",
        "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                results = selfie_segmentation.process(rgb_frame)\n",
        "\n",
        "                # Create the binary mask for segmentation\n",
        "                condition = results.segmentation_mask > 0.5\n",
        "\n",
        "                # Apply the mask to set background pixels to black\n",
        "                output_frame = np.zeros_like(frame)  # Start with a black background\n",
        "                output_frame[condition] = frame[condition]  # Set person pixels from original frame\n",
        "\n",
        "                # Write the frame with the black background\n",
        "                out.write(output_frame)\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        print(f\"Background-removed video saved at {output_video_path}\")\n",
        "\n",
        "    # Process each video in the input directory\n",
        "    for video_file in os.listdir(input_videos_dir):\n",
        "        if video_file.endswith('.mp4'):\n",
        "            input_video_path = os.path.join(input_videos_dir, video_file)\n",
        "            output_video_path = os.path.join(bg_removed_videos_dir, video_file.replace('.mp4', '_no_bg.mp4'))\n",
        "            remove_background(input_video_path, output_video_path)\n",
        "\n",
        "\n",
        "\n",
        "    bg_removed_videos_dir = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/bg_removed_videos\"\n",
        "    bg_removed_videos = sorted([os.path.join(bg_removed_videos_dir, f) for f in os.listdir(bg_removed_videos_dir) if f.endswith('_no_bg.mp4')])\n",
        "\n",
        "    def trim_videos(video_paths):\n",
        "        # Load video clips and get their durations\n",
        "        clips = []\n",
        "        durations = []\n",
        "\n",
        "        for path in video_paths:\n",
        "            try:\n",
        "                clip = VideoFileClip(path)\n",
        "                clips.append(clip)\n",
        "                durations.append(clip.duration)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading video {path}: {e}\")\n",
        "\n",
        "        # Ensure we have valid clips to work with\n",
        "        if not clips:\n",
        "            print(\"No valid clips loaded for trimming.\")\n",
        "            return\n",
        "\n",
        "        # Find the minimum duration in seconds\n",
        "        min_duration = min(durations)\n",
        "        print(f\"Minimum duration found: {min_duration:.3f} seconds\")\n",
        "\n",
        "        # Trim each video to the minimum duration\n",
        "        for i, clip in enumerate(clips):\n",
        "            try:\n",
        "                current_duration = clip.duration\n",
        "\n",
        "                # Check if the current video needs trimming\n",
        "                if current_duration > min_duration:\n",
        "                    # Trim video to the minimum duration\n",
        "                    trimmed_clip = clip.subclip(0, min_duration)\n",
        "\n",
        "                    # Write the trimmed video back to the same file\n",
        "                    trimmed_clip.write_videofile(video_paths[i], codec='libx264', audio_codec='aac', remove_temp=True)\n",
        "                    trimmed_clip.close()  # Close the trimmed clip\n",
        "                    print(f\"Trimmed {video_paths[i]} to {min_duration:.3f} seconds\")\n",
        "                else:\n",
        "                    print(f\"{video_paths[i]} is already at or below the minimum duration.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error trimming video {video_paths[i]}: {e}\")\n",
        "\n",
        "        # Close all original clips\n",
        "        for clip in clips:\n",
        "            clip.close()\n",
        "\n",
        "\n",
        "\n",
        "    # video concatenation Module\n",
        "\n",
        "    # Paths\n",
        "    bg_removed_videos_dir = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/bg_removed_videos\"\n",
        "    merged_video_path = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/merged_video/merged_video.mp4\"\n",
        "\n",
        "\n",
        "    # Get all background-removed video files\n",
        "    bg_removed_videos = sorted([os.path.join(bg_removed_videos_dir, f) for f in os.listdir(bg_removed_videos_dir) if f.endswith('.mp4')])\n",
        "\n",
        "    # Load video clips\n",
        "    clips = [VideoFileClip(video).resize((256, 256)) for video in bg_removed_videos]\n",
        "\n",
        "    # Concatenate clips horizontally\n",
        "    final_clip = clips_array([[clips[0], clips[1], clips[2]]])  # Adjust this if you have more or fewer clips\n",
        "\n",
        "    # Write the output video\n",
        "    final_clip.write_videofile(merged_video_path, codec='libx264', fps=30)\n",
        "\n",
        "    print(f\"Concatenated video saved at: {merged_video_path}\")\n",
        "\n",
        "    # Define paths\n",
        "    merged_video_path = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/merged_video/merged_video.mp4\"\n",
        "    merged_video_frames_path = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/merged_video/merged_videoframes/\"\n",
        "    background_video_tracker = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/background_video_tracker.txt\"\n",
        "    background_video_dir = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/age_specific_motion/\"\n",
        "    background_frames_path = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/framesbackgroundvideo/\"\n",
        "    output_frames_path = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/outputvideoframes/\"\n",
        "    final_output_video_path = \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/final_outputvideo/final_output_video.mp4\"\n",
        "\n",
        "\n",
        "    # Utility: Clear frames from a folder without deleting the folder itself\n",
        "    def clear_folder(folder_path):\n",
        "        if os.path.exists(folder_path):\n",
        "            for file in os.listdir(folder_path):\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                    os.unlink(file_path)  # Remove files or links\n",
        "                elif os.path.isdir(file_path):\n",
        "                    shutil.rmtree(file_path)  # Remove subdirectories\n",
        "        else:\n",
        "            os.makedirs(folder_path)  # Create the folder if it doesn't exist\n",
        "\n",
        "\n",
        "    # Dynamic background video selection\n",
        "    def load_background_video():\n",
        "        # Read current tracker value\n",
        "        with open(background_video_tracker, \"r\") as file:\n",
        "            current_index = int(file.read().strip())\n",
        "\n",
        "        # Update tracker to next video\n",
        "        new_index = (current_index % 6) + 1\n",
        "        with open(background_video_tracker, \"w\") as file:\n",
        "            file.write(str(new_index))\n",
        "\n",
        "        # Select background video path\n",
        "        background_video_name = f\"backgroundvideo{new_index}.mp4\"\n",
        "        background_video_path = os.path.join(background_video_dir, background_video_name)\n",
        "        print(f\"Selected Background Video: {background_video_path}\")\n",
        "        return background_video_path\n",
        "\n",
        "\n",
        "    # Clear folders before processing\n",
        "    clear_folder(merged_video_frames_path)\n",
        "    clear_folder(background_frames_path)\n",
        "    clear_folder(output_frames_path)\n",
        "\n",
        "    # Step 1: Extract frames from a video\n",
        "    def extract_frames_from_video(video_path, output_frames_path):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_count = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame_filename = os.path.join(output_frames_path, f'frame_{frame_count:04d}.png')\n",
        "            cv2.imwrite(frame_filename, frame)\n",
        "            frame_count += 1\n",
        "        cap.release()\n",
        "        print(f\"Extracted {frame_count} frames from {video_path}.\")\n",
        "        return frame_count\n",
        "\n",
        "    # Step 2: Replace black background with transparency\n",
        "    def replace_black_background_with_transparency(frames_path):\n",
        "        frame_files = sorted(os.listdir(frames_path))\n",
        "        for frame_file in frame_files:\n",
        "            frame_path = os.path.join(frames_path, frame_file)\n",
        "            frame = cv2.imread(frame_path)\n",
        "            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "            lower_black = np.array([0, 0, 0])\n",
        "            upper_black = np.array([180, 255, 30])\n",
        "            mask = cv2.inRange(hsv, lower_black, upper_black)\n",
        "            frame_bgra = cv2.cvtColor(frame, cv2.COLOR_BGR2BGRA)\n",
        "            frame_bgra[mask != 0] = [0, 0, 0, 0]\n",
        "            cv2.imwrite(frame_path, frame_bgra)\n",
        "\n",
        "    # Step 3: Overlay transparent frames onto background frames\n",
        "    def overlay_frames(transparent_frames_path, background_frames_path, output_frames_path):\n",
        "        transparent_frame_files = sorted(os.listdir(transparent_frames_path))\n",
        "        background_frame_files = sorted(os.listdir(background_frames_path))\n",
        "\n",
        "        # Align frame counts by trimming extra frames from the longer video\n",
        "        min_frame_count = min(len(transparent_frame_files), len(background_frame_files))\n",
        "        transparent_frame_files = transparent_frame_files[:min_frame_count]\n",
        "        background_frame_files = background_frame_files[:min_frame_count]\n",
        "\n",
        "        for i, frame_file in enumerate(transparent_frame_files):\n",
        "            transparent_frame = cv2.imread(os.path.join(transparent_frames_path, frame_file), cv2.IMREAD_UNCHANGED)\n",
        "            background_frame = cv2.imread(os.path.join(background_frames_path, background_frame_files[i]))\n",
        "\n",
        "            if transparent_frame.shape[2] == 4:  # Check for alpha channel\n",
        "                alpha_channel = transparent_frame[:, :, 3] / 255.0\n",
        "                for c in range(3):  # Blend RGB channels\n",
        "                    background_frame[:, :, c] = background_frame[:, :, c] * (1 - alpha_channel) + transparent_frame[:, :, c] * alpha_channel\n",
        "\n",
        "            output_frame_path = os.path.join(output_frames_path, f'output_frame_{i:04d}.png')\n",
        "            cv2.imwrite(output_frame_path, background_frame)\n",
        "\n",
        "        print(f\"Overlay complete. {min_frame_count} frames processed.\")\n",
        "\n",
        "    # Step 4: Combine frames into a video\n",
        "    def create_video_from_frames(frames_path, output_video_path, fps=30):\n",
        "        frame_files = sorted(os.listdir(frames_path))\n",
        "        if not frame_files:\n",
        "            raise ValueError(\"No frames found to create a video.\")\n",
        "\n",
        "        first_frame = cv2.imread(os.path.join(frames_path, frame_files[0]))\n",
        "        height, width, _ = first_frame.shape\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "        for frame_file in frame_files:\n",
        "            frame = cv2.imread(os.path.join(frames_path, frame_file))\n",
        "            video_writer.write(frame)\n",
        "\n",
        "        video_writer.release()\n",
        "        print(f\"Video saved at {output_video_path}.\")\n",
        "\n",
        "    # Execute the pipeline\n",
        "    background_video_path = load_background_video()  # Load dynamic background video\n",
        "    merged_frame_count = extract_frames_from_video(merged_video_path, merged_video_frames_path)\n",
        "    replace_black_background_with_transparency(merged_video_frames_path)\n",
        "    background_frame_count = extract_frames_from_video(background_video_path, background_frames_path)\n",
        "    overlay_frames(merged_video_frames_path, background_frames_path, output_frames_path)\n",
        "    create_video_from_frames(output_frames_path, final_output_video_path, fps=30)\n",
        "\n",
        "    return age_message, \"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/final_outputvideo/final_output_video.mp4\"\n",
        "\n",
        "\n",
        "# UI Layout\n",
        "# UI Layout\n",
        "def create_ui():\n",
        "    # Define UI elements\n",
        "    with gr.Blocks(css=\".gradio-container {background-color: black; color: white;}\") as demo:\n",
        "        # Title\n",
        "        gr.Markdown(\"<h1 style='text-align: center; color: white;'>Generate Age-specific Animation using AI</h1>\")\n",
        "\n",
        "        # Display Image (Introductory image preview)\n",
        "        gr.Image(\n",
        "            value=\"/content/drive/MyDrive/agebased_motion_application_deeplearning_project/websiteimagesample.png\",\n",
        "            label=\"Animation Preview\",\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "        # Upload Section\n",
        "        gr.Markdown(\"<h2 style='text-align: center; color: white;'>Upload 3 Images!</h2>\")\n",
        "\n",
        "        # Image Upload Boxes with Preview\n",
        "        with gr.Row():\n",
        "            image1 = gr.Image(label=\"Image 1\", interactive=True, type=\"filepath\")\n",
        "            image2 = gr.Image(label=\"Image 2\", interactive=True, type=\"filepath\")\n",
        "            image3 = gr.Image(label=\"Image 3\", interactive=True, type=\"filepath\")\n",
        "\n",
        "        # Generate Video Button\n",
        "        generate_button = gr.Button(\"Generate Video\")\n",
        "\n",
        "        # Detected Ages Textbox\n",
        "        output_ages = gr.Textbox(label=\"Detected Ages\", interactive=False)\n",
        "\n",
        "        # Playable Video Section\n",
        "        output_video = gr.Video(label=\"Generated Video\")\n",
        "\n",
        "\n",
        "\n",
        "        # Button Click Action\n",
        "        generate_button.click(\n",
        "            fn=agespecificmotion,  # Call the external function\n",
        "            inputs=[image1, image2, image3],  # Pass inputs to the function\n",
        "            outputs=[output_ages, output_video]  # Display both ages and video\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Launch the UI\n",
        "create_ui().launch()\n"
      ],
      "metadata": {
        "id": "trhtvU2Gm1qp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}